<!DOCTYPE html><html lang="en" data-theme="dark"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><!-- Primary Meta Tags --><title>NVIDIA Unveils OmniVinci: A Research-Focused Multimodal LLM • Dev|Journal</title><meta name="description" content="NVIDIA Introduces OmniVinci: A Leap Towards Human-like AI Perception NVIDIA has announced the development of OmniVinci, a novel large language model (LLM)…"><meta name="keywords" content="software architecture, backend development, microservices, Java, Python, Spring Boot, technical blog"><meta name="author" content="El Mehdi Arezki"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://earezki.com/ai-news/2025-10-28-nvidia-introduces-omnivinci-a-research-only-llm-for-cross-modal-understanding/"><meta property="og:title" content="NVIDIA Unveils OmniVinci: A Research-Focused Multimodal LLM • Dev|Journal"><meta property="og:site_name" content="Dev|Journal"><meta property="og:description" content="NVIDIA Introduces OmniVinci: A Leap Towards Human-like AI Perception NVIDIA has announced the development of OmniVinci, a novel large language model (LLM)…"><meta property="og:image" content="https://earezki.com/assets/og-image-default.jpg"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://earezki.com/ai-news/2025-10-28-nvidia-introduces-omnivinci-a-research-only-llm-for-cross-modal-understanding/"><meta name="twitter:title" content="NVIDIA Unveils OmniVinci: A Research-Focused Multimodal LLM • Dev|Journal"><meta name="twitter:description" content="NVIDIA Introduces OmniVinci: A Leap Towards Human-like AI Perception NVIDIA has announced the development of OmniVinci, a novel large language model (LLM)…"><meta name="twitter:image" content="https://earezki.com/assets/og-image-default.jpg"><meta name="twitter:creator" content="@earezki"><!-- Canonical and Indexing --><link rel="canonical" href="https://earezki.com/ai-news/2025-10-28-nvidia-introduces-omnivinci-a-research-only-llm-for-cross-modal-understanding/"><!-- Favicons --><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="icon" type="image/x-icon" href="/favicon.ico"><!-- RSS Feed --><link rel="alternate" type="application/rss+xml" title="RSS" href="/rss.xml"><!-- Analytics: Google Analytics (Legacy UA) --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-161447264-1"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);} 
      gtag('js', new Date()); 
      gtag('config', 'UA-161447264-1');
    </script><!-- Analytics: Umami --><script defer src="https://cloud.umami.is/script.js" data-website-id="4a26531d-1053-4f79-97a6-06a1366aff91"></script><!-- Theme Persistence (runs before page render to prevent flash) --><script>
      (function() {
        const theme = localStorage.getItem('theme');
        if (theme) { 
          document.documentElement.setAttribute('data-theme', theme); 
        }
      })();
    </script><!-- Structured Data (JSON-LD) --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"NVIDIA Unveils OmniVinci: A Research-Focused Multimodal LLM","datePublished":"2025-10-28T00:00:00.000Z","dateModified":"2025-10-28T00:00:00.000Z","author":{"@type":"Person","name":"AREZKI El Mehdi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://earezki.com/ai-news/2025-10-28-nvidia-introduces-omnivinci-a-research-only-llm-for-cross-modal-understanding/"},"description":"NVIDIA Introduces OmniVinci: A Leap Towards Human-like AI Perception NVIDIA has announced the development of OmniVinci, a novel large language model (LLM)…"}</script><link rel="stylesheet" href="/_astro/_slug_.C8vOLobN.css">
<style>.toc[data-astro-cid-xvrfupwn]{position:sticky;top:90px;max-height:calc(100vh - 120px);overflow:auto;padding:1rem 1rem 1.2rem;background:var(--color-bg-alt);border:1px solid var(--color-border);border-radius:var(--radius-md);font-size:.8rem;line-height:1.3}.toc-title[data-astro-cid-xvrfupwn]{font-weight:600;font-size:.75rem;text-transform:uppercase;letter-spacing:.08em;margin-bottom:.6rem;color:var(--color-text-alt)}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;margin:0;padding:0;display:flex;flex-direction:column;gap:.35rem}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{text-decoration:none;color:var(--color-text-alt);transition:color var(--transition)}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--color-accent)}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-3]{margin-left:.75rem}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-4]{margin-left:1.4rem}@media (max-width: 1080px){.toc[data-astro-cid-xvrfupwn]{display:none}}.breadcrumbs[data-astro-cid-ilhxcym7]{margin:0 0 1.5rem;font-size:.85rem}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;padding:0;margin:0;display:flex;flex-wrap:wrap;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{display:flex;align-items:center;gap:.5rem}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--color-text-alt);text-decoration:none;transition:color var(--transition)}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--color-accent);text-decoration:underline}.breadcrumbs[data-astro-cid-ilhxcym7] .separator[data-astro-cid-ilhxcym7]{color:var(--color-text-alt);opacity:.5;user-select:none}.breadcrumbs[data-astro-cid-ilhxcym7] .current[data-astro-cid-ilhxcym7]{color:var(--color-text);font-weight:500}.related-posts[data-astro-cid-dpgbfi7r]{margin:3rem 0;padding-top:2rem;border-top:2px solid var(--color-border)}.related-posts[data-astro-cid-dpgbfi7r] h2[data-astro-cid-dpgbfi7r]{font-size:1.5rem;margin:0 0 1.5rem;color:var(--color-text)}.related-posts-grid[data-astro-cid-dpgbfi7r]{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:1.5rem}.related-post-card[data-astro-cid-dpgbfi7r]{background:var(--color-bg-alt);border:1px solid var(--color-border);border-radius:var(--radius-md);padding:1.25rem;transition:all var(--transition)}.related-post-card[data-astro-cid-dpgbfi7r]:hover{border-color:var(--color-accent);transform:translateY(-2px);box-shadow:var(--shadow-md)}.related-post-link[data-astro-cid-dpgbfi7r]{text-decoration:none;color:inherit;display:block}.related-post-card[data-astro-cid-dpgbfi7r] h3[data-astro-cid-dpgbfi7r]{margin:0 0 .75rem;font-size:1.1rem;font-weight:600;color:var(--color-text);line-height:1.3}.related-post-link[data-astro-cid-dpgbfi7r]:hover h3[data-astro-cid-dpgbfi7r]{color:var(--color-accent)}.related-excerpt[data-astro-cid-dpgbfi7r]{font-size:.85rem;color:var(--color-text-alt);line-height:1.5;margin:0 0 .75rem}.related-meta[data-astro-cid-dpgbfi7r]{font-size:.75rem;color:var(--color-text-alt);display:flex;gap:.5rem;align-items:center}@media (max-width: 680px){.related-posts-grid[data-astro-cid-dpgbfi7r]{grid-template-columns:1fr}}
.post-layout[data-astro-cid-rkg3zjxi]{display:grid;grid-template-columns:260px 1fr;gap:2.5rem}.sidebar-left[data-astro-cid-rkg3zjxi]{position:relative}@media (max-width: 1080px){.post-layout[data-astro-cid-rkg3zjxi]{grid-template-columns:1fr}}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]{display:flex;gap:.75rem;padding:.9rem 1.1rem;margin:0 auto 2rem;max-width:780px;background:linear-gradient(135deg,#667eea1a,#764ba21a,#f093fb1a);border:2px solid transparent;border-radius:var(--radius-md);position:relative;background-clip:padding-box;font-size:.85rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]:before{content:"";position:absolute;inset:-2px;border-radius:var(--radius-md);padding:2px;background:var(--ai-gradient-border);background-size:200% 200%;animation:gradient-rotate 3s linear infinite;-webkit-mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);-webkit-mask-composite:xor;mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);mask-composite:exclude;pointer-events:none}html[data-theme=dark] .ai-disclaimer-article[data-astro-cid-rkg3zjxi]{background:linear-gradient(135deg,#667eea26,#764ba226,#f093fb26)}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] svg[data-astro-cid-rkg3zjxi]{margin-top:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] strong[data-astro-cid-rkg3zjxi]{color:var(--color-text);display:block;margin-bottom:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] div[data-astro-cid-rkg3zjxi]{line-height:1.5;color:var(--color-text-alt)}
</style><script type="module" src="/_astro/hoisted.2U5tn40l.js"></script></head> <body> <div id="readingProgress" role="progressbar" aria-label="Reading progress" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100" style="position:fixed;left:0;top:0;height:3px;background:linear-gradient(90deg,#2563eb,#9333ea);width:0;z-index:999;transition:width .15s ease;"></div>  <!-- Site Header --> <header class="site-header container"> <div class="logo-wrap"> <a class="logo" href="/" aria-label="Dev|Journal Home">
Dev|Journal
</a> </div> <nav class="main-nav" aria-label="Main navigation"> <a href="/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" style="vertical-align: -2px; margin-right: 4px;"> <path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path> <polyline points="9 22 9 12 15 12 15 22"></polyline> </svg>
Home
</a> <a href="/ai-news/" class="ai-news-link"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h-9.5a.5.5 0 0 0-.5.5.5.5 0 0 1-1 0 .5.5 0 0 0-.5-.5H1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2z"></path> <path d="M7 15v4a2 2 0 0 0 2 2h6a2 2 0 0 0 2-2v-4"></path> </svg>
AI News
</a> <a href="/tags/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2H2v10l9.29 9.29c.94.94 2.48.94 3.42 0l6.58-6.58c.94-.94.94-2.48 0-3.42L12 2z"></path> <circle cx="7" cy="7" r="1.5"></circle> </svg>
Tags
</a> <a href="/about/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" style="vertical-align: -2px; margin-right: 4px;"> <circle cx="12" cy="12" r="10"></circle> <path d="M12 16v-4"></path> <path d="M12 8h.01"></path> </svg>
About
</a> <button id="themeToggle" class="theme-toggle" aria-label="Toggle dark mode"> <span class="theme-icon" aria-hidden="true">☾</span> <span class="theme-text">dark</span> </button> </nav> </header> <!-- Main Content --> <main class="container content-area">  <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7>  <a href="/" data-astro-cid-ilhxcym7>Home</a> <span class="separator" aria-hidden="true" data-astro-cid-ilhxcym7>/</span>  </li><li data-astro-cid-ilhxcym7>  <a href="/ai-news/" data-astro-cid-ilhxcym7>Ai News</a> <span class="separator" aria-hidden="true" data-astro-cid-ilhxcym7>/</span>  </li><li data-astro-cid-ilhxcym7> <span class="current" aria-current="page" data-astro-cid-ilhxcym7>NVIDIA Unveils OmniVinci: A Research-Focused Multimodal LLM</span> </li> </ol> </nav>  <div class="ai-disclaimer-article" data-astro-cid-rkg3zjxi> <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="flex-shrink: 0;" data-astro-cid-rkg3zjxi> <circle cx="12" cy="12" r="10" data-astro-cid-rkg3zjxi></circle> <line x1="12" y1="8" x2="12" y2="12" data-astro-cid-rkg3zjxi></line> <line x1="12" y1="16" x2="12.01" y2="16" data-astro-cid-rkg3zjxi></line> </svg> <div data-astro-cid-rkg3zjxi> <strong data-astro-cid-rkg3zjxi>AI-Generated Content:</strong> This article was created with AI assistance. Please verify important information and check original references.
</div> </div> <div class="post-layout" data-astro-cid-rkg3zjxi> <aside class="sidebar-left" data-astro-cid-rkg3zjxi> <nav class="toc" aria-label="Table of Contents" data-astro-cid-xvrfupwn> <div class="toc-title" data-astro-cid-xvrfupwn>On this page</div> <ul data-astro-cid-xvrfupwn> <li class="d-2" data-astro-cid-xvrfupwn> <a href="#nvidia-introduces-omnivinci-a-leap-towards-human-like-ai-perception" data-astro-cid-xvrfupwn> NVIDIA Introduces OmniVinci: A Leap Towards Human-like AI Perception </a> </li><li class="d-3" data-astro-cid-xvrfupwn> <a href="#key-features-and-architecture" data-astro-cid-xvrfupwn> Key Features and Architecture </a> </li><li class="d-3" data-astro-cid-xvrfupwn> <a href="#performance-and-training-data" data-astro-cid-xvrfupwn> Performance and Training Data </a> </li><li class="d-3" data-astro-cid-xvrfupwn> <a href="#applications-and-accessibility" data-astro-cid-xvrfupwn> Applications and Accessibility </a> </li><li class="d-3" data-astro-cid-xvrfupwn> <a href="#controversy-surrounding-the-license" data-astro-cid-xvrfupwn> Controversy Surrounding the License </a> </li> </ul> </nav>  </aside> <article class="post" data-astro-cid-rkg3zjxi> <header class="post-header" data-astro-cid-rkg3zjxi> <h1 data-astro-cid-rkg3zjxi>NVIDIA Unveils OmniVinci: A Research-Focused Multimodal LLM</h1> <div class="meta" data-astro-cid-rkg3zjxi> <time datetime="2025-10-28T00:00:00.000Z" data-astro-cid-rkg3zjxi>Tue Oct 28 2025</time> <span data-astro-cid-rkg3zjxi>• 2 min read</span> </div> <div class="tag-row" data-astro-cid-rkg3zjxi><a class="post-tag ai-news-tag" href="/tags/ai-news/" data-astro-cid-rkg3zjxi>AI News</a><a class="post-tag " href="/tags/large-language-models/" data-astro-cid-rkg3zjxi>Large language models</a><a class="post-tag " href="/tags/ml-%26-data-engineering/" data-astro-cid-rkg3zjxi>ML &amp; Data Engineering</a></div> </header> <div class="post-body prose" data-astro-cid-rkg3zjxi> <h2 id="nvidia-introduces-omnivinci-a-leap-towards-human-like-ai-perception">NVIDIA Introduces OmniVinci: A Leap Towards Human-like AI Perception</h2>
<p>NVIDIA has announced the development of OmniVinci, a novel large language model (LLM) focused on understanding and reasoning across diverse input modalities, including text, vision, audio, and robotics data. This project, spearheaded by NVIDIA Research, aims to advance the field of machine intelligence by enabling models to interpret the world in a more comprehensive and human-like manner.  OmniVinci distinguishes itself through its architectural innovations and a large-scale synthetic data pipeline, achieving impressive results with a relatively small training dataset.</p>
<h3 id="key-features-and-architecture">Key Features and Architecture</h3>
<p>OmniVinci’s core innovation lies in its ability to integrate information from multiple sources.  It achieves this through three key components:</p>
<ul>
<li><strong>OmniAlignNet:</strong> This component aligns vision and audio embeddings within a shared latent space, facilitating cross-modal understanding.</li>
<li><strong>Temporal Embedding Grouping:</strong> This module captures the temporal relationships between video and audio signals, crucial for understanding dynamic content.</li>
<li><strong>Constrained Rotary Time Embedding:</strong> This component encodes absolute temporal information, enabling synchronization of multi-modal inputs.</li>
</ul>
<p>These components are built upon a new data synthesis engine that generated over 24 million single- and multi-modal conversations.</p>
<h3 id="performance-and-training-data">Performance and Training Data</h3>
<p>A significant aspect of OmniVinci is its efficiency in training.  It achieved notable performance improvements using only 0.2 trillion training tokens, significantly less than the 0.5 trillion required by Qwen2.5-Omni. The research paper highlights the following performance gains on key benchmarks:</p>
<ul>
<li><strong>DailyOmni:</strong> +19.05 improvement for cross-modal understanding.</li>
<li><strong>MMAR (Audio Tasks):</strong> +1.7 improvement.</li>
<li><strong>Video-MME (Vision Performance):</strong> +3.9 improvement.</li>
</ul>
<p>These results suggest that modalities reinforce each other, leading to enhanced perception and reasoning capabilities when models are trained on combined data.</p>
<h3 id="applications-and-accessibility">Applications and Accessibility</h3>
<p>NVIDIA envisions OmniVinci having broad applications in various domains, including:</p>
<ul>
<li><strong>Robotics:</strong>  Improving decision-making accuracy and reducing latency through cross-modal context.</li>
<li><strong>Medical Imaging:</strong> Enhancing diagnostic capabilities by integrating visual and textual information.</li>
<li><strong>Smart Factory Automation:</strong> Optimizing processes through the analysis of multi-modal data streams.</li>
</ul>
<p>Access to OmniVinci is currently restricted to those who accept NVIDIA’s OneWay Noncommercial License.  The model is accessible through Hugging Face, providing setup scripts and examples for inference on video, audio, and image data using Transformers. The codebase is built upon NVILA, NVIDIA’s multi-modal foundation, and supports full GPU acceleration for real-time applications.</p>
<h3 id="controversy-surrounding-the-license">Controversy Surrounding the License</h3>
<p>The release of OmniVinci under a non-commercial license has generated debate within the AI community. Critics, such as data researcher Julià Agramunt, argue that this approach represents “digital feudalism,” where NVIDIA retains commercial rights while the community contributes to the model’s improvement without sharing in the profits.  Concerns have also been raised regarding the accessibility of the model at launch, with some users reporting difficulties in gaining access to benchmark results.</p>
<p>Source: <a href="https://www.infoq.com/news/2025/10/nvidia-omnivinci/">https://www.infoq.com/news/2025/10/nvidia-omnivinci/</a></p> </div>  <section class="related-posts" data-astro-cid-dpgbfi7r> <h2 data-astro-cid-dpgbfi7r>Related Posts</h2> <div class="related-posts-grid" data-astro-cid-dpgbfi7r> <article class="related-post-card" data-astro-cid-dpgbfi7r> <a href="/ai-news/2025-10-20-google-introduces-llm-evalkit-to-bring-order-and-metrics-to-prompt-engineering/" class="related-post-link" data-astro-cid-dpgbfi7r> <h3 data-astro-cid-dpgbfi7r>Google Launches LLM-Evalkit for Data-Driven Prompt Engineering</h3> <p class="related-excerpt" data-astro-cid-dpgbfi7r>Google Introduces LLM-Evalkit: A Framework for Measuring Prompt Engineering Google has launched LLM-Evalkit, an…</p> <div class="related-meta" data-astro-cid-dpgbfi7r> <time datetime="2025-10-20T00:00:00.000Z" data-astro-cid-dpgbfi7r> Oct 20, 2025 </time> <span data-astro-cid-dpgbfi7r>•</span> <span data-astro-cid-dpgbfi7r>2 min read</span> </div> </a> </article><article class="related-post-card" data-astro-cid-dpgbfi7r> <a href="/ai-news/2025-10-22-deepseek-ai-unveils-deepseek-ocr-vision-based-context-compression-redefines-long-text-processing/" class="related-post-link" data-astro-cid-dpgbfi7r> <h3 data-astro-cid-dpgbfi7r>DeepSeek AI Introduces DeepSeek-OCR: A Novel Approach to Context Compression for LLMs</h3> <p class="related-excerpt" data-astro-cid-dpgbfi7r>DeepSeek-OCR: A New Paradigm for Long-Text Processing DeepSeek AI has unveiled DeepSeek-OCR, an open-source system…</p> <div class="related-meta" data-astro-cid-dpgbfi7r> <time datetime="2025-10-22T00:00:00.000Z" data-astro-cid-dpgbfi7r> Oct 22, 2025 </time> <span data-astro-cid-dpgbfi7r>•</span> <span data-astro-cid-dpgbfi7r>2 min read</span> </div> </a> </article><article class="related-post-card" data-astro-cid-dpgbfi7r> <a href="/ai-news/2025-10-24-pytorch-monarch-simplifies-distributed-ai-workflows-with-a-single-controller-model/" class="related-post-link" data-astro-cid-dpgbfi7r> <h3 data-astro-cid-dpgbfi7r>Meta&#39;s PyTorch Monarch Simplifies Distributed AI Workflows</h3> <p class="related-excerpt" data-astro-cid-dpgbfi7r>Monarch: Simplifying Distributed AI with a Single-Controller Model Meta&#39;s PyTorch team has introduced Monarch, an…</p> <div class="related-meta" data-astro-cid-dpgbfi7r> <time datetime="2025-10-24T00:00:00.000Z" data-astro-cid-dpgbfi7r> Oct 24, 2025 </time> <span data-astro-cid-dpgbfi7r>•</span> <span data-astro-cid-dpgbfi7r>2 min read</span> </div> </a> </article> </div> </section>  </article> </div> <div class="back-link" data-astro-cid-rkg3zjxi><a href="/ai-news/" data-astro-cid-rkg3zjxi>← Back to AI News</a></div>  </main> <!-- Site Footer --> <footer class="site-footer container"> <p>
© 2025 AREZKI El Mehdi •
<a href="https://github.com/earezki">GitHub</a> •
<a href="/rss.xml">RSS</a> •
<a href="/sitemap-index.xml">Sitemap</a> </p> </footer> <!-- Theme Toggle Script -->  <!-- Article Read/Unread Tracking --> <script>
      /**
       * Tracks which articles have been read using localStorage
       * Updates post card styling on list pages
       */
      (function() {
        const STORAGE_KEY = 'readArticles';
        
        /**
         * Gets list of read article URLs from localStorage
         */
        function getReadArticles() {
          try {
            const data = localStorage.getItem(STORAGE_KEY);
            return data ? JSON.parse(data) : [];
          } catch (e) {
            console.error('Failed to get read articles:', e);
            return [];
          }
        }
        
        /**
         * Marks an article URL as read
         */
        function markArticleAsRead(url) {
          try {
            const readArticles = getReadArticles();
            if (!readArticles.includes(url)) {
              readArticles.push(url);
              localStorage.setItem(STORAGE_KEY, JSON.stringify(readArticles));
            }
          } catch (e) {
            console.error('Failed to mark article as read:', e);
          }
        }
        
        /**
         * Checks if an article has been read
         */
        function isArticleRead(url) {
          const readArticles = getReadArticles();
          return readArticles.includes(url);
        }
        
        /**
         * Updates post card styling on list pages based on read status
         */
        function updatePostCards() {
          const postCards = document.querySelectorAll('.post-card[data-article-url]');
          
          postCards.forEach(function(card) {
            const url = card.getAttribute('data-article-url');
            if (!url) return;
            
            if (isArticleRead(url)) {
              card.classList.add('read');
              card.classList.remove('unread');
            } else {
              card.classList.add('unread');
              card.classList.remove('read');
            }
          });
        }
        
        /**
         * Marks current article as read (on article detail pages)
         */
        function markCurrentArticleAsRead() {
          const isArticlePage = document.querySelector('.post-body');
          if (isArticlePage) {
            const currentPath = window.location.pathname;
            markArticleAsRead(currentPath);
          }
        }
        
        /**
         * Initialize tracking system
         */
        function init() {
          updatePostCards();
          markCurrentArticleAsRead();
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', init);
        
        // Handle back/forward navigation (bfcache)
        window.addEventListener('pageshow', function(event) {
          if (event.persisted) {
            init();
          }
        });
      })();
    </script> <!-- Code Copy Functionality --> <script>
      /**
       * Adds copy buttons to all code blocks
       */
      (function() {
        /**
         * Creates a copy button element
         */
        function createCopyButton() {
          const button = document.createElement('button');
          button.className = 'copy-code-button';
          button.setAttribute('aria-label', 'Copy code to clipboard');
          button.innerHTML = `
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
              <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
              <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
            </svg>
            <span>Copy</span>
          `;
          return button;
        }
        
        /**
         * Creates a success checkmark icon
         */
        function createCheckIcon() {
          return `
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
              <polyline points="20 6 9 17 4 12"></polyline>
            </svg>
            <span>Copied!</span>
          `;
        }
        
        /**
         * Copies text to clipboard
         */
        async function copyToClipboard(text) {
          try {
            await navigator.clipboard.writeText(text);
            return true;
          } catch (err) {
            // Fallback for older browsers
            const textArea = document.createElement('textarea');
            textArea.value = text;
            textArea.style.position = 'fixed';
            textArea.style.left = '-999999px';
            document.body.appendChild(textArea);
            textArea.select();
            try {
              document.execCommand('copy');
              document.body.removeChild(textArea);
              return true;
            } catch (e) {
              document.body.removeChild(textArea);
              return false;
            }
          }
        }
        
        /**
         * Handles copy button click
         */
        async function handleCopyClick(button, codeBlock) {
          const code = codeBlock.textContent || '';
          const success = await copyToClipboard(code);
          
          if (success) {
            const originalHTML = button.innerHTML;
            button.classList.add('copied');
            button.innerHTML = createCheckIcon();
            
            setTimeout(function() {
              button.classList.remove('copied');
              button.innerHTML = originalHTML;
            }, 2000);
          }
        }
        
        /**
         * Adds copy buttons to all code blocks
         */
        function addCopyButtons() {
          const codeBlocks = document.querySelectorAll('.post-body pre');
          
          codeBlocks.forEach(function(pre) {
            // Skip if button already exists
            if (pre.querySelector('.copy-code-button')) {
              return;
            }
            
            const button = createCopyButton();
            const codeBlock = pre.querySelector('code');
            
            if (codeBlock) {
              button.addEventListener('click', function() {
                handleCopyClick(button, codeBlock);
              });
              
              pre.appendChild(button);
            }
          });
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', addCopyButtons);
      })();
    </script> </body> </html> 