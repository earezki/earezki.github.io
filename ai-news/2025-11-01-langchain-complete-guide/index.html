<!DOCTYPE html><html lang="en" data-theme="dark"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>LangChain Complete Guide: Building Production-Ready LLM Applications • Dev|Journal</title><meta name="description" content="LangChain Complete Guide: Building Production-Ready LLM Applications LangChain is the leading framework for building applications with Large Language Models.…"><meta name="keywords" content="software architecture, backend development, microservices, Java, Python, Spring Boot, technical blog"><meta name="author" content="El Mehdi Arezki"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://earezki.com/ai-news/2025-11-01-langchain-complete-guide/"><meta property="og:title" content="LangChain Complete Guide: Building Production-Ready LLM Applications • Dev|Journal"><meta property="og:site_name" content="Dev|Journal"><meta property="og:description" content="LangChain Complete Guide: Building Production-Ready LLM Applications LangChain is the leading framework for building applications with Large Language Models.…"><meta property="og:image" content="https://earezki.com/assets/og-image-default.jpg"><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:url" content="https://earezki.com/ai-news/2025-11-01-langchain-complete-guide/"><meta name="twitter:title" content="LangChain Complete Guide: Building Production-Ready LLM Applications • Dev|Journal"><meta name="twitter:description" content="LangChain Complete Guide: Building Production-Ready LLM Applications LangChain is the leading framework for building applications with Large Language Models.…"><meta name="twitter:image" content="https://earezki.com/assets/og-image-default.jpg"><meta name="twitter:creator" content="@earezki"><link rel="canonical" href="https://earezki.com/ai-news/2025-11-01-langchain-complete-guide/"><link rel="alternate" type="application/rss+xml" title="RSS" href="/rss.xml"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-161447264-1"></script><script defer src="https://cloud.umami.is/script.js" data-website-id="4a26531d-1053-4f79-97a6-06a1366aff91"></script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"LangChain Complete Guide: Building Production-Ready LLM Applications","datePublished":"2025-11-01T16:00:00.000Z","dateModified":"2025-11-01T16:00:00.000Z","author":{"@type":"Person","name":"AREZKI El Mehdi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://earezki.com/ai-news/2025-11-01-langchain-complete-guide/"},"description":"LangChain Complete Guide: Building Production-Ready LLM Applications LangChain is the leading framework for building applications with Large Language Models.…"}</script><link rel="stylesheet" href="/_astro/_slug_.CDWqF8AB.css">
<style>.post-layout[data-astro-cid-rkg3zjxi]{display:grid;grid-template-columns:260px 1fr;gap:2.5rem}.sidebar-left[data-astro-cid-rkg3zjxi]{position:relative}@media (max-width: 1080px){.post-layout[data-astro-cid-rkg3zjxi]{grid-template-columns:1fr}}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]{display:flex;gap:.75rem;padding:.9rem 1.1rem;margin:0 auto 2rem;max-width:780px;background:linear-gradient(135deg,#667eea1a,#764ba21a,#f093fb1a);border:2px solid transparent;border-radius:var(--radius-md);position:relative;background-clip:padding-box;font-size:.85rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]:before{content:"";position:absolute;inset:-2px;border-radius:var(--radius-md);padding:2px;background:var(--ai-gradient-border);background-size:200% 200%;animation:gradient-rotate 3s linear infinite;-webkit-mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);-webkit-mask-composite:xor;mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);mask-composite:exclude;pointer-events:none}html[data-theme=dark] .ai-disclaimer-article[data-astro-cid-rkg3zjxi]{background:linear-gradient(135deg,#667eea26,#764ba226,#f093fb26)}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] svg[data-astro-cid-rkg3zjxi]{margin-top:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] strong[data-astro-cid-rkg3zjxi]{color:var(--color-text);display:block;margin-bottom:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] div[data-astro-cid-rkg3zjxi]{line-height:1.5;color:var(--color-text-alt)}
.toc[data-astro-cid-xvrfupwn]{position:sticky;top:90px;max-height:calc(100vh - 120px);overflow:auto;padding:1rem 1rem 1.2rem;background:var(--color-bg-alt);border:1px solid var(--color-border);border-radius:var(--radius-md);font-size:.8rem;line-height:1.3}.toc-title[data-astro-cid-xvrfupwn]{font-weight:600;font-size:.75rem;text-transform:uppercase;letter-spacing:.08em;margin-bottom:.6rem;color:var(--color-text-alt)}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;margin:0;padding:0;display:flex;flex-direction:column;gap:.35rem}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{text-decoration:none;color:var(--color-text-alt)}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--color-accent)}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-3]{margin-left:.75rem}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-4]{margin-left:1.4rem}@media (max-width: 1080px){.toc[data-astro-cid-xvrfupwn]{display:none}}
</style><script type="module" src="/_astro/hoisted.CwSnpN4J.js"></script></head> <body> <div id="readingProgress" style="position:fixed;left:0;top:0;height:3px;background:linear-gradient(90deg,#2563eb,#9333ea);width:0;z-index:999;transition:width .15s ease;"></div>  <header class="site-header container"> <div class="logo-wrap"> <a class="logo" href="/">Dev|Journal</a> </div> <nav class="main-nav"> <a href="/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path> <polyline points="9 22 9 12 15 12 15 22"></polyline> </svg>
Home
</a> <a href="/ai-news/" class="ai-news-link"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h-9.5a.5.5 0 0 0-.5.5.5.5 0 0 1-1 0 .5.5 0 0 0-.5-.5H1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2z"></path> <path d="M7 15v4a2 2 0 0 0 2 2h6a2 2 0 0 0 2-2v-4"></path> </svg>
AI News
</a> <a href="/tags/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2H2v10l9.29 9.29c.94.94 2.48.94 3.42 0l6.58-6.58c.94-.94.94-2.48 0-3.42L12 2z"></path> <circle cx="7" cy="7" r="1.5"></circle> </svg>
Tags
</a> <a href="/about/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <circle cx="12" cy="12" r="10"></circle> <path d="M12 16v-4"></path> <path d="M12 8h.01"></path> </svg>
About
</a> <button id="themeToggle" class="theme-toggle" aria-label="Toggle dark mode"> <span class="theme-icon">☾</span> <span class="theme-text">dark</span> </button> </nav> </header> <main class="container content-area">  <div class="ai-disclaimer-article" data-astro-cid-rkg3zjxi> <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="flex-shrink: 0;" data-astro-cid-rkg3zjxi> <circle cx="12" cy="12" r="10" data-astro-cid-rkg3zjxi></circle> <line x1="12" y1="8" x2="12" y2="12" data-astro-cid-rkg3zjxi></line> <line x1="12" y1="16" x2="12.01" y2="16" data-astro-cid-rkg3zjxi></line> </svg> <div data-astro-cid-rkg3zjxi> <strong data-astro-cid-rkg3zjxi>AI-Generated Content:</strong> This article was created with AI assistance. Please verify important information and check original references.
</div> </div> <div class="post-layout" data-astro-cid-rkg3zjxi> <aside class="sidebar-left" data-astro-cid-rkg3zjxi> <nav class="toc" aria-label="Table of Contents" data-astro-cid-xvrfupwn> <div class="toc-title" data-astro-cid-xvrfupwn>On this page</div> <ul data-astro-cid-xvrfupwn> <li class="d-2" data-astro-cid-xvrfupwn><a href="#table-of-contents" data-astro-cid-xvrfupwn>Table of Contents</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#langchain-fundamentals-fundamentals" data-astro-cid-xvrfupwn>LangChain Fundamentals {#fundamentals}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-installation-and-setup" data-astro-cid-xvrfupwn>1. Installation and Setup</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-basic-llm-usage" data-astro-cid-xvrfupwn>2. Basic LLM Usage</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#3-core-components-overview" data-astro-cid-xvrfupwn>3. Core Components Overview</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#llm-integration-llm-integration" data-astro-cid-xvrfupwn>LLM Integration {#llm-integration}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-multiple-llm-providers" data-astro-cid-xvrfupwn>1. Multiple LLM Providers</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-token-management-and-cost-optimization" data-astro-cid-xvrfupwn>2. Token Management and Cost Optimization</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#prompt-engineering-prompts" data-astro-cid-xvrfupwn>Prompt Engineering {#prompts}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-prompt-templates" data-astro-cid-xvrfupwn>1. Prompt Templates</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-advanced-prompt-techniques" data-astro-cid-xvrfupwn>2. Advanced Prompt Techniques</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#chains-and-lcel-chains" data-astro-cid-xvrfupwn>Chains and LCEL {#chains}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-langchain-expression-language-lcel" data-astro-cid-xvrfupwn>1. LangChain Expression Language (LCEL)</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-complex-chains" data-astro-cid-xvrfupwn>2. Complex Chains</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#agents-and-tools-agents" data-astro-cid-xvrfupwn>Agents and Tools {#agents}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-creating-custom-tools" data-astro-cid-xvrfupwn>1. Creating Custom Tools</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-agent-types-and-usage" data-astro-cid-xvrfupwn>2. Agent Types and Usage</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#3-custom-agent-implementation" data-astro-cid-xvrfupwn>3. Custom Agent Implementation</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#memory-systems-memory" data-astro-cid-xvrfupwn>Memory Systems {#memory}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-memory-types" data-astro-cid-xvrfupwn>1. Memory Types</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-memory-in-chains" data-astro-cid-xvrfupwn>2. Memory in Chains</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#rag-retrieval-augmented-generation-rag" data-astro-cid-xvrfupwn>RAG (Retrieval Augmented Generation) {#rag}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-basic-rag-implementation" data-astro-cid-xvrfupwn>1. Basic RAG Implementation</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-advanced-rag-techniques" data-astro-cid-xvrfupwn>2. Advanced RAG Techniques</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#vector-stores-vector-stores" data-astro-cid-xvrfupwn>Vector Stores {#vector-stores}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-vector-store-options" data-astro-cid-xvrfupwn>1. Vector Store Options</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-vector-store-operations" data-astro-cid-xvrfupwn>2. Vector Store Operations</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#evaluation-and-monitoring-evaluation" data-astro-cid-xvrfupwn>Evaluation and Monitoring {#evaluation}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-evaluating-rag-systems" data-astro-cid-xvrfupwn>1. Evaluating RAG Systems</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-monitoring-and-logging" data-astro-cid-xvrfupwn>2. Monitoring and Logging</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#production-deployment-deployment" data-astro-cid-xvrfupwn>Production Deployment {#deployment}</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-fastapi-integration" data-astro-cid-xvrfupwn>1. FastAPI Integration</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-caching-and-optimization" data-astro-cid-xvrfupwn>2. Caching and Optimization</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#3-production-best-practices" data-astro-cid-xvrfupwn>3. Production Best Practices</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#conclusion" data-astro-cid-xvrfupwn>Conclusion</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#resources" data-astro-cid-xvrfupwn>Resources</a></li> </ul> </nav>  </aside> <article class="post" data-astro-cid-rkg3zjxi> <header class="post-header" data-astro-cid-rkg3zjxi> <h1 data-astro-cid-rkg3zjxi>LangChain Complete Guide: Building Production-Ready LLM Applications</h1> <div class="meta" data-astro-cid-rkg3zjxi> <time datetime="2025-11-01T16:00:00.000Z" data-astro-cid-rkg3zjxi>Sat Nov 01 2025</time> <span data-astro-cid-rkg3zjxi>• 15 min read</span> </div> <div class="tag-row" data-astro-cid-rkg3zjxi><span class="tag" data-astro-cid-rkg3zjxi>Python</span><span class="tag" data-astro-cid-rkg3zjxi>AI</span><span class="tag" data-astro-cid-rkg3zjxi>LangChain</span><span class="tag" data-astro-cid-rkg3zjxi>LLM</span><span class="tag" data-astro-cid-rkg3zjxi>Machine Learning</span></div> </header> <div class="post-body prose" data-astro-cid-rkg3zjxi> <h1 id="langchain-complete-guide-building-production-ready-llm-applications">LangChain Complete Guide: Building Production-Ready LLM Applications</h1>
<p>LangChain is the leading framework for building applications with Large Language Models. This comprehensive guide covers everything from basics to production deployment, with real-world examples and best practices.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#fundamentals">LangChain Fundamentals</a></li>
<li><a href="#llm-integration">LLM Integration</a></li>
<li><a href="#prompts">Prompt Engineering</a></li>
<li><a href="#chains">Chains and LCEL</a></li>
<li><a href="#agents">Agents and Tools</a></li>
<li><a href="#memory">Memory Systems</a></li>
<li><a href="#rag">RAG (Retrieval Augmented Generation)</a></li>
<li><a href="#vector-stores">Vector Stores</a></li>
<li><a href="#evaluation">Evaluation and Monitoring</a></li>
<li><a href="#deployment">Production Deployment</a></li>
</ol>
<h2 id="langchain-fundamentals-fundamentals">LangChain Fundamentals {#fundamentals}</h2>
<h3 id="1-installation-and-setup">1. Installation and Setup</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Install LangChain and dependencies</span></span>
<span class="line"><span style="color:#E6EDF3">pip install langchain langchain</span><span style="color:#FF7B72">-</span><span style="color:#E6EDF3">openai langchain</span><span style="color:#FF7B72">-</span><span style="color:#E6EDF3">community</span></span>
<span class="line"><span style="color:#E6EDF3">pip install chromadb  </span><span style="color:#8B949E"># Vector store</span></span>
<span class="line"><span style="color:#E6EDF3">pip install faiss</span><span style="color:#FF7B72">-</span><span style="color:#E6EDF3">cpu  </span><span style="color:#8B949E"># Alternative vector store</span></span>
<span class="line"><span style="color:#E6EDF3">pip install sentence</span><span style="color:#FF7B72">-</span><span style="color:#E6EDF3">transformers  </span><span style="color:#8B949E"># Embeddings</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Basic imports</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_openai </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatOpenAI, OpenAIEmbeddings</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.messages </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> HumanMessage, SystemMessage</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.prompts </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatPromptTemplate</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.output_parsers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> StrOutputParser</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> os</span></span>
<span class="line"><span style="color:#E6EDF3">os.environ[</span><span style="color:#A5D6FF">"OPENAI_API_KEY"</span><span style="color:#E6EDF3">] </span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF"> "your-api-key"</span></span>
<span class="line"></span></code></pre>
<h3 id="2-basic-llm-usage">2. Basic LLM Usage</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Initialize LLM</span></span>
<span class="line"><span style="color:#E6EDF3">llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatOpenAI(</span></span>
<span class="line"><span style="color:#FFA657">    model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"gpt-4"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    temperature</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">0.7</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    max_tokens</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">500</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Simple invocation</span></span>
<span class="line"><span style="color:#E6EDF3">response </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> llm.invoke(</span><span style="color:#A5D6FF">"What is LangChain?"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">print</span><span style="color:#E6EDF3">(response.content)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># With message history</span></span>
<span class="line"><span style="color:#E6EDF3">messages </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> [</span></span>
<span class="line"><span style="color:#E6EDF3">    SystemMessage(</span><span style="color:#FFA657">content</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"You are a helpful AI assistant."</span><span style="color:#E6EDF3">),</span></span>
<span class="line"><span style="color:#E6EDF3">    HumanMessage(</span><span style="color:#FFA657">content</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"What is Python?"</span><span style="color:#E6EDF3">),</span></span>
<span class="line"><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">response </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> llm.invoke(messages)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Streaming responses</span></span>
<span class="line"><span style="color:#FF7B72">for</span><span style="color:#E6EDF3"> chunk </span><span style="color:#FF7B72">in</span><span style="color:#E6EDF3"> llm.stream(</span><span style="color:#A5D6FF">"Write a poem about coding"</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#79C0FF">    print</span><span style="color:#E6EDF3">(chunk.content, </span><span style="color:#FFA657">end</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">""</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">flush</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Async usage</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> asyncio</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> async_generate</span><span style="color:#E6EDF3">():</span></span>
<span class="line"><span style="color:#E6EDF3">    response </span><span style="color:#FF7B72">=</span><span style="color:#FF7B72"> await</span><span style="color:#E6EDF3"> llm.ainvoke(</span><span style="color:#A5D6FF">"Tell me a joke"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> response.content</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> asyncio.run(async_generate())</span></span>
<span class="line"></span></code></pre>
<h3 id="3-core-components-overview">3. Core Components Overview</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.runnables </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> RunnablePassthrough</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.prompts </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> PromptTemplate</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.output_parsers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> JsonOutputParser</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Prompt Template</span></span>
<span class="line"><span style="color:#E6EDF3">prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> PromptTemplate(</span></span>
<span class="line"><span style="color:#FFA657">    template</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Tell me a </span><span style="color:#FF7B72">{adjective}</span><span style="color:#A5D6FF"> joke about </span><span style="color:#FF7B72">{topic}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    input_variables</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span><span style="color:#A5D6FF">"adjective"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"topic"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Output Parser</span></span>
<span class="line"><span style="color:#E6EDF3">parser </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Chain components together using LCEL</span></span>
<span class="line"><span style="color:#E6EDF3">chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> parser</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Invoke chain</span></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain.invoke({</span></span>
<span class="line"><span style="color:#A5D6FF">    "adjective"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"funny"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">    "topic"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"programming"</span></span>
<span class="line"><span style="color:#E6EDF3">})</span></span>
<span class="line"><span style="color:#79C0FF">print</span><span style="color:#E6EDF3">(result)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Batch processing</span></span>
<span class="line"><span style="color:#E6EDF3">results </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain.batch([</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"adjective"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"funny"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"topic"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Python"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"adjective"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"silly"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"topic"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"JavaScript"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#E6EDF3">])</span></span>
<span class="line"></span></code></pre>
<h2 id="llm-integration-llm-integration">LLM Integration {#llm-integration}</h2>
<h3 id="1-multiple-llm-providers">1. Multiple LLM Providers</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># OpenAI</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_openai </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatOpenAI</span></span>
<span class="line"><span style="color:#E6EDF3">openai_llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatOpenAI(</span><span style="color:#FFA657">model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"gpt-4"</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">temperature</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">0</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Anthropic Claude</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_anthropic </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatAnthropic</span></span>
<span class="line"><span style="color:#E6EDF3">claude_llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatAnthropic(</span><span style="color:#FFA657">model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"claude-3-opus-20240229"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Google PaLM</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_google_genai </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatGoogleGenerativeAI</span></span>
<span class="line"><span style="color:#E6EDF3">palm_llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatGoogleGenerativeAI(</span><span style="color:#FFA657">model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"gemini-pro"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Local models with Ollama</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.llms </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Ollama</span></span>
<span class="line"><span style="color:#E6EDF3">local_llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Ollama(</span><span style="color:#FFA657">model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"llama2"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># HuggingFace models</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.llms </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> HuggingFaceHub</span></span>
<span class="line"><span style="color:#E6EDF3">hf_llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> HuggingFaceHub(</span></span>
<span class="line"><span style="color:#FFA657">    repo_id</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"google/flan-t5-xl"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    model_kwargs</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"temperature"</span><span style="color:#E6EDF3">: </span><span style="color:#79C0FF">0.5</span><span style="color:#E6EDF3">}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># LLM abstraction for switching providers</span></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> LLMFactory</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#D2A8FF">    @</span><span style="color:#79C0FF">staticmethod</span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> create_llm</span><span style="color:#E6EDF3">(provider: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs):</span></span>
<span class="line"><span style="color:#E6EDF3">        providers </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> {</span></span>
<span class="line"><span style="color:#A5D6FF">            "openai"</span><span style="color:#E6EDF3">: ChatOpenAI,</span></span>
<span class="line"><span style="color:#A5D6FF">            "anthropic"</span><span style="color:#E6EDF3">: ChatAnthropic,</span></span>
<span class="line"><span style="color:#A5D6FF">            "google"</span><span style="color:#E6EDF3">: ChatGoogleGenerativeAI,</span></span>
<span class="line"><span style="color:#A5D6FF">            "ollama"</span><span style="color:#E6EDF3">: Ollama,</span></span>
<span class="line"><span style="color:#E6EDF3">        }</span></span>
<span class="line"><span style="color:#E6EDF3">        </span></span>
<span class="line"><span style="color:#E6EDF3">        llm_class </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> providers.get(provider)</span></span>
<span class="line"><span style="color:#FF7B72">        if</span><span style="color:#FF7B72"> not</span><span style="color:#E6EDF3"> llm_class:</span></span>
<span class="line"><span style="color:#FF7B72">            raise</span><span style="color:#79C0FF"> ValueError</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Unknown provider: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">provider</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">        </span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#E6EDF3"> llm_class(</span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Usage</span></span>
<span class="line"><span style="color:#E6EDF3">llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> LLMFactory.create_llm(</span></span>
<span class="line"><span style="color:#A5D6FF">    "openai"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"gpt-4"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    temperature</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">0</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-token-management-and-cost-optimization">2. Token Management and Cost Optimization</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.callbacks </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> get_openai_callback</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_openai </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatOpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatOpenAI(</span><span style="color:#FFA657">model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"gpt-3.5-turbo"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Track token usage and cost</span></span>
<span class="line"><span style="color:#FF7B72">with</span><span style="color:#E6EDF3"> get_openai_callback() </span><span style="color:#FF7B72">as</span><span style="color:#E6EDF3"> cb:</span></span>
<span class="line"><span style="color:#E6EDF3">    result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> llm.invoke(</span><span style="color:#A5D6FF">"Write a long story about AI"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">    print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Total Tokens: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">cb.total_tokens</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">    print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Prompt Tokens: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">cb.prompt_tokens</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">    print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Completion Tokens: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">cb.completion_tokens</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">    print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Total Cost (USD): $</span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">cb.total_cost</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Token counting</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> tiktoken </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> encoding_for_model</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> count_tokens</span><span style="color:#E6EDF3">(text: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">, model: </span><span style="color:#79C0FF">str</span><span style="color:#FF7B72"> =</span><span style="color:#A5D6FF"> "gpt-4"</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">int</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#E6EDF3">    encoding </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> encoding_for_model(model)</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#79C0FF"> len</span><span style="color:#E6EDF3">(encoding.encode(text))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">text </span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF"> "Hello, how are you?"</span></span>
<span class="line"><span style="color:#E6EDF3">tokens </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> count_tokens(text)</span></span>
<span class="line"><span style="color:#79C0FF">print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Tokens: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">tokens</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Cost estimation before API call</span></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> estimate_cost</span><span style="color:#E6EDF3">(prompt: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">, max_tokens: </span><span style="color:#79C0FF">int</span><span style="color:#FF7B72"> =</span><span style="color:#79C0FF"> 500</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">float</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#E6EDF3">    input_tokens </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> count_tokens(prompt)</span></span>
<span class="line"><span style="color:#E6EDF3">    total_tokens </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> input_tokens </span><span style="color:#FF7B72">+</span><span style="color:#E6EDF3"> max_tokens</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#8B949E">    # GPT-4 pricing (example)</span></span>
<span class="line"><span style="color:#E6EDF3">    cost_per_1k </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> 0.03</span><span style="color:#8B949E">  # Input</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> (total_tokens </span><span style="color:#FF7B72">/</span><span style="color:#79C0FF"> 1000</span><span style="color:#E6EDF3">) </span><span style="color:#FF7B72">*</span><span style="color:#E6EDF3"> cost_per_1k</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">estimated </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> estimate_cost(</span><span style="color:#A5D6FF">"Write a detailed analysis..."</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Estimated cost: $</span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">estimated</span><span style="color:#FF7B72">:.4f}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h2 id="prompt-engineering-prompts">Prompt Engineering {#prompts}</h2>
<h3 id="1-prompt-templates">1. Prompt Templates</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.prompts </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    ChatPromptTemplate,</span></span>
<span class="line"><span style="color:#E6EDF3">    PromptTemplate,</span></span>
<span class="line"><span style="color:#E6EDF3">    FewShotPromptTemplate,</span></span>
<span class="line"><span style="color:#E6EDF3">    MessagesPlaceholder</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Simple prompt template</span></span>
<span class="line"><span style="color:#E6EDF3">simple_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> PromptTemplate(</span></span>
<span class="line"><span style="color:#FFA657">    template</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"What is the capital of </span><span style="color:#FF7B72">{country}</span><span style="color:#A5D6FF">?"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    input_variables</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span><span style="color:#A5D6FF">"country"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Chat prompt template</span></span>
<span class="line"><span style="color:#E6EDF3">chat_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_messages([</span></span>
<span class="line"><span style="color:#E6EDF3">    (</span><span style="color:#A5D6FF">"system"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"You are a </span><span style="color:#FF7B72">{role}</span><span style="color:#A5D6FF"> expert."</span><span style="color:#E6EDF3">),</span></span>
<span class="line"><span style="color:#E6EDF3">    (</span><span style="color:#A5D6FF">"user"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"</span><span style="color:#FF7B72">{question}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chat_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain.invoke({</span></span>
<span class="line"><span style="color:#A5D6FF">    "role"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Python programming"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">    "question"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"What are decorators?"</span></span>
<span class="line"><span style="color:#E6EDF3">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Few-shot prompting</span></span>
<span class="line"><span style="color:#E6EDF3">examples </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> [</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span></span>
<span class="line"><span style="color:#A5D6FF">        "input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"happy"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">        "output"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"sad"</span></span>
<span class="line"><span style="color:#E6EDF3">    },</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span></span>
<span class="line"><span style="color:#A5D6FF">        "input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"tall"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">        "output"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"short"</span></span>
<span class="line"><span style="color:#E6EDF3">    },</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span></span>
<span class="line"><span style="color:#A5D6FF">        "input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"hot"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">        "output"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"cold"</span></span>
<span class="line"><span style="color:#E6EDF3">    }</span></span>
<span class="line"><span style="color:#E6EDF3">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">example_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> PromptTemplate(</span></span>
<span class="line"><span style="color:#FFA657">    template</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Input: </span><span style="color:#FF7B72">{input}\n</span><span style="color:#A5D6FF">Output: </span><span style="color:#FF7B72">{output}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    input_variables</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"output"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">few_shot_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> FewShotPromptTemplate(</span></span>
<span class="line"><span style="color:#FFA657">    examples</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">examples,</span></span>
<span class="line"><span style="color:#FFA657">    example_prompt</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">example_prompt,</span></span>
<span class="line"><span style="color:#FFA657">    prefix</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Give the opposite of every word:"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    suffix</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Input: </span><span style="color:#FF7B72">{word}\n</span><span style="color:#A5D6FF">Output:"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    input_variables</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span><span style="color:#A5D6FF">"word"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Dynamic few-shot with example selector</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.example_selectors </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> SemanticSimilarityExampleSelector</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Chroma</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">example_selector </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> SemanticSimilarityExampleSelector.from_examples(</span></span>
<span class="line"><span style="color:#E6EDF3">    examples,</span></span>
<span class="line"><span style="color:#E6EDF3">    OpenAIEmbeddings(),</span></span>
<span class="line"><span style="color:#E6EDF3">    Chroma,</span></span>
<span class="line"><span style="color:#FFA657">    k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">2</span><span style="color:#8B949E">  # Select 2 most similar examples</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">dynamic_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> FewShotPromptTemplate(</span></span>
<span class="line"><span style="color:#FFA657">    example_selector</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">example_selector,</span></span>
<span class="line"><span style="color:#FFA657">    example_prompt</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">example_prompt,</span></span>
<span class="line"><span style="color:#FFA657">    prefix</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Give the opposite:"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    suffix</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Input: </span><span style="color:#FF7B72">{word}\n</span><span style="color:#A5D6FF">Output:"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    input_variables</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span><span style="color:#A5D6FF">"word"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-advanced-prompt-techniques">2. Advanced Prompt Techniques</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Chain of Thought prompting</span></span>
<span class="line"><span style="color:#E6EDF3">cot_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span><span style="color:#A5D6FF">"""</span></span>
<span class="line"><span style="color:#A5D6FF">Solve this problem step by step:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Problem: </span><span style="color:#FF7B72">{problem}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Let's approach this systematically:</span></span>
<span class="line"><span style="color:#A5D6FF">1. Understand the problem</span></span>
<span class="line"><span style="color:#A5D6FF">2. Break it down into steps</span></span>
<span class="line"><span style="color:#A5D6FF">3. Solve each step</span></span>
<span class="line"><span style="color:#A5D6FF">4. Provide the final answer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Solution:</span></span>
<span class="line"><span style="color:#A5D6FF">"""</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Self-consistency prompting</span></span>
<span class="line"><span style="color:#FF7B72">async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> self_consistency</span><span style="color:#E6EDF3">(question: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">, n: </span><span style="color:#79C0FF">int</span><span style="color:#FF7B72"> =</span><span style="color:#79C0FF"> 5</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#A5D6FF">    """Generate multiple responses and pick most common"""</span></span>
<span class="line"><span style="color:#E6EDF3">    responses </span><span style="color:#FF7B72">=</span><span style="color:#FF7B72"> await</span><span style="color:#E6EDF3"> asyncio.gather(</span><span style="color:#FF7B72">*</span><span style="color:#E6EDF3">[</span></span>
<span class="line"><span style="color:#E6EDF3">        llm.ainvoke(question) </span><span style="color:#FF7B72">for</span><span style="color:#E6EDF3"> _ </span><span style="color:#FF7B72">in</span><span style="color:#79C0FF"> range</span><span style="color:#E6EDF3">(n)</span></span>
<span class="line"><span style="color:#E6EDF3">    ])</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#8B949E">    # Find most common response (simplified)</span></span>
<span class="line"><span style="color:#FF7B72">    from</span><span style="color:#E6EDF3"> collections </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Counter</span></span>
<span class="line"><span style="color:#E6EDF3">    answers </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> [r.content </span><span style="color:#FF7B72">for</span><span style="color:#E6EDF3"> r </span><span style="color:#FF7B72">in</span><span style="color:#E6EDF3"> responses]</span></span>
<span class="line"><span style="color:#E6EDF3">    most_common </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Counter(answers).most_common(</span><span style="color:#79C0FF">1</span><span style="color:#E6EDF3">)[</span><span style="color:#79C0FF">0</span><span style="color:#E6EDF3">][</span><span style="color:#79C0FF">0</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> most_common</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># ReAct prompting (Reasoning + Acting)</span></span>
<span class="line"><span style="color:#E6EDF3">react_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span><span style="color:#A5D6FF">"""</span></span>
<span class="line"><span style="color:#A5D6FF">Answer the following question using this format:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Thought: Think about what to do</span></span>
<span class="line"><span style="color:#A5D6FF">Action: The action to take</span></span>
<span class="line"><span style="color:#A5D6FF">Observation: The result of the action</span></span>
<span class="line"><span style="color:#A5D6FF">... (repeat Thought/Action/Observation as needed)</span></span>
<span class="line"><span style="color:#A5D6FF">Thought: Final conclusion</span></span>
<span class="line"><span style="color:#A5D6FF">Answer: The final answer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Question: </span><span style="color:#FF7B72">{question}</span></span>
<span class="line"><span style="color:#A5D6FF">"""</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Prompt with output structuring</span></span>
<span class="line"><span style="color:#E6EDF3">structured_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span><span style="color:#A5D6FF">"""</span></span>
<span class="line"><span style="color:#A5D6FF">Extract information from the text and return as JSON:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Text: </span><span style="color:#FF7B72">{text}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Return JSON with these fields:</span></span>
<span class="line"><span style="color:#A5D6FF">- name: Person's name</span></span>
<span class="line"><span style="color:#A5D6FF">- age: Person's age  </span></span>
<span class="line"><span style="color:#A5D6FF">- occupation: Person's occupation</span></span>
<span class="line"><span style="color:#A5D6FF">- location: Person's location</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">JSON:</span></span>
<span class="line"><span style="color:#A5D6FF">"""</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.output_parsers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> JsonOutputParser</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">parser </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> JsonOutputParser()</span></span>
<span class="line"><span style="color:#E6EDF3">chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> structured_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> parser</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain.invoke({</span></span>
<span class="line"><span style="color:#A5D6FF">    "text"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"John Smith is a 35-year-old software engineer living in San Francisco."</span></span>
<span class="line"><span style="color:#E6EDF3">})</span></span>
<span class="line"><span style="color:#79C0FF">print</span><span style="color:#E6EDF3">(result)  </span><span style="color:#8B949E"># {"name": "John Smith", "age": 35, ...}</span></span>
<span class="line"></span></code></pre>
<h2 id="chains-and-lcel-chains">Chains and LCEL {#chains}</h2>
<h3 id="1-langchain-expression-language-lcel">1. LangChain Expression Language (LCEL)</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.runnables </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    RunnablePassthrough,</span></span>
<span class="line"><span style="color:#E6EDF3">    RunnableParallel,</span></span>
<span class="line"><span style="color:#E6EDF3">    RunnableLambda</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Basic chain</span></span>
<span class="line"><span style="color:#E6EDF3">prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span><span style="color:#A5D6FF">"Tell me about </span><span style="color:#FF7B72">{topic}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Chain with intermediate steps</span></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> extract_keywords</span><span style="color:#E6EDF3">(text: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">) -> list[</span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">]:</span></span>
<span class="line"><span style="color:#8B949E">    # Extract keywords from text</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> text.lower().split()[:</span><span style="color:#79C0FF">3</span><span style="color:#E6EDF3">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">keyword_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    prompt</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> llm</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> RunnableLambda(extract_keywords)</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">keywords </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> keyword_chain.invoke({</span><span style="color:#A5D6FF">"topic"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Python programming"</span><span style="color:#E6EDF3">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Parallel execution</span></span>
<span class="line"><span style="color:#E6EDF3">parallel_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> RunnableParallel({</span></span>
<span class="line"><span style="color:#A5D6FF">    "summary"</span><span style="color:#E6EDF3">: prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> StrOutputParser(),</span></span>
<span class="line"><span style="color:#A5D6FF">    "keywords"</span><span style="color:#E6EDF3">: keyword_chain,</span></span>
<span class="line"><span style="color:#A5D6FF">    "length"</span><span style="color:#E6EDF3">: RunnableLambda(</span><span style="color:#FF7B72">lambda</span><span style="color:#E6EDF3"> x: </span><span style="color:#79C0FF">len</span><span style="color:#E6EDF3">(x[</span><span style="color:#A5D6FF">"topic"</span><span style="color:#E6EDF3">]))</span></span>
<span class="line"><span style="color:#E6EDF3">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">results </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> parallel_chain.invoke({</span><span style="color:#A5D6FF">"topic"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Machine Learning"</span><span style="color:#E6EDF3">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Branching chain</span></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> route_question</span><span style="color:#E6EDF3">(input_dict):</span></span>
<span class="line"><span style="color:#E6EDF3">    question </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> input_dict[</span><span style="color:#A5D6FF">"question"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#FF7B72">    if</span><span style="color:#A5D6FF"> "python"</span><span style="color:#FF7B72"> in</span><span style="color:#E6EDF3"> question.lower():</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#A5D6FF"> "python_chain"</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#A5D6FF"> "general_chain"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">python_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span></span>
<span class="line"><span style="color:#A5D6FF">    "As a Python expert: </span><span style="color:#FF7B72">{question}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">general_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Answer: </span><span style="color:#FF7B72">{question}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">branch_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> {</span></span>
<span class="line"><span style="color:#A5D6FF">    "python_chain"</span><span style="color:#E6EDF3">: python_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm,</span></span>
<span class="line"><span style="color:#A5D6FF">    "general_chain"</span><span style="color:#E6EDF3">: general_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm</span></span>
<span class="line"><span style="color:#E6EDF3">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Conditional routing</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.runnables </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> RunnableBranch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">conditional_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> RunnableBranch(</span></span>
<span class="line"><span style="color:#E6EDF3">    (</span><span style="color:#FF7B72">lambda</span><span style="color:#E6EDF3"> x: </span><span style="color:#A5D6FF">"python"</span><span style="color:#FF7B72"> in</span><span style="color:#E6EDF3"> x[</span><span style="color:#A5D6FF">"question"</span><span style="color:#E6EDF3">].lower(), python_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm),</span></span>
<span class="line"><span style="color:#E6EDF3">    (</span><span style="color:#FF7B72">lambda</span><span style="color:#E6EDF3"> x: </span><span style="color:#A5D6FF">"javascript"</span><span style="color:#FF7B72"> in</span><span style="color:#E6EDF3"> x[</span><span style="color:#A5D6FF">"question"</span><span style="color:#E6EDF3">].lower(), general_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm),</span></span>
<span class="line"><span style="color:#E6EDF3">    general_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm  </span><span style="color:#8B949E"># default</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-complex-chains">2. Complex Chains</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Map-Reduce chain for document summarization</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.chains.combine_documents.stuff </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> create_stuff_documents_chain</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.chains </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> MapReduceDocumentsChain</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Map step: Summarize each document</span></span>
<span class="line"><span style="color:#E6EDF3">map_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Summarize this document:</span><span style="color:#FF7B72">\n\n{page_content}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">map_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> map_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Reduce step: Combine summaries</span></span>
<span class="line"><span style="color:#E6EDF3">reduce_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Combine these summaries into a final summary:</span><span style="color:#FF7B72">\n\n{summaries}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">reduce_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> reduce_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Sequential chain with multiple steps</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.chains </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> SequentialChain</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Step 1: Generate outline</span></span>
<span class="line"><span style="color:#E6EDF3">outline_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    ChatPromptTemplate.from_template(</span><span style="color:#A5D6FF">"Create an outline for: </span><span style="color:#FF7B72">{topic}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> llm</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Step 2: Write content based on outline</span></span>
<span class="line"><span style="color:#E6EDF3">content_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    ChatPromptTemplate.from_template(</span></span>
<span class="line"><span style="color:#A5D6FF">        "Write content for this outline:</span><span style="color:#FF7B72">\n{outline}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"><span style="color:#E6EDF3">    )</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> llm</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Combine steps</span></span>
<span class="line"><span style="color:#E6EDF3">full_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> outline_chain </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> content_chain</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Transform chain for data processing</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.schema </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Document</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> load_documents</span><span style="color:#E6EDF3">() -> list[Document]:</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> [</span></span>
<span class="line"><span style="color:#E6EDF3">        Document(</span><span style="color:#FFA657">page_content</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Document 1 content"</span><span style="color:#E6EDF3">),</span></span>
<span class="line"><span style="color:#E6EDF3">        Document(</span><span style="color:#FFA657">page_content</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Document 2 content"</span><span style="color:#E6EDF3">),</span></span>
<span class="line"><span style="color:#E6EDF3">    ]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Process each document</span></span>
<span class="line"><span style="color:#E6EDF3">process_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    RunnableLambda(load_documents)</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> RunnableLambda(</span><span style="color:#FF7B72">lambda</span><span style="color:#E6EDF3"> docs: [</span></span>
<span class="line"><span style="color:#E6EDF3">        (map_prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm).invoke({</span><span style="color:#A5D6FF">"page_content"</span><span style="color:#E6EDF3">: doc.page_content})</span></span>
<span class="line"><span style="color:#FF7B72">        for</span><span style="color:#E6EDF3"> doc </span><span style="color:#FF7B72">in</span><span style="color:#E6EDF3"> docs</span></span>
<span class="line"><span style="color:#E6EDF3">    ])</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h2 id="agents-and-tools-agents">Agents and Tools {#agents}</h2>
<h3 id="1-creating-custom-tools">1. Creating Custom Tools</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.tools </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Tool, StructuredTool</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.tools </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> tool</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> pydantic </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> BaseModel, Field</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Simple function tool</span></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> search_api</span><span style="color:#E6EDF3">(query: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#A5D6FF">    """Search the internet for information"""</span></span>
<span class="line"><span style="color:#8B949E">    # Implement actual search logic</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#FF7B72"> f</span><span style="color:#A5D6FF">"Search results for: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">query</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">search_tool </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Tool(</span></span>
<span class="line"><span style="color:#FFA657">    name</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Search"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    func</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">search_api,</span></span>
<span class="line"><span style="color:#FFA657">    description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Searches the internet for information"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Decorator-based tool</span></span>
<span class="line"><span style="color:#D2A8FF">@tool</span></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> calculator</span><span style="color:#E6EDF3">(expression: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">float</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#A5D6FF">    """Evaluates mathematical expressions"""</span></span>
<span class="line"><span style="color:#FF7B72">    try</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#79C0FF"> eval</span><span style="color:#E6EDF3">(expression)</span></span>
<span class="line"><span style="color:#FF7B72">    except</span><span style="color:#79C0FF"> Exception</span><span style="color:#FF7B72"> as</span><span style="color:#E6EDF3"> e:</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#FF7B72"> f</span><span style="color:#A5D6FF">"Error: </span><span style="color:#FF7B72">{</span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">(e)</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Structured tool with typed inputs</span></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> CalculatorInput</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">BaseModel</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#E6EDF3">    a: </span><span style="color:#79C0FF">float</span><span style="color:#FF7B72"> =</span><span style="color:#E6EDF3"> Field(</span><span style="color:#FFA657">description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"First number"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">    b: </span><span style="color:#79C0FF">float</span><span style="color:#FF7B72"> =</span><span style="color:#E6EDF3"> Field(</span><span style="color:#FFA657">description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Second number"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">    operation: </span><span style="color:#79C0FF">str</span><span style="color:#FF7B72"> =</span><span style="color:#E6EDF3"> Field(</span><span style="color:#FFA657">description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Operation: add, subtract, multiply, divide"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> calculator_func</span><span style="color:#E6EDF3">(a: </span><span style="color:#79C0FF">float</span><span style="color:#E6EDF3">, b: </span><span style="color:#79C0FF">float</span><span style="color:#E6EDF3">, operation: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">float</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#E6EDF3">    operations </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> {</span></span>
<span class="line"><span style="color:#A5D6FF">        "add"</span><span style="color:#E6EDF3">: a </span><span style="color:#FF7B72">+</span><span style="color:#E6EDF3"> b,</span></span>
<span class="line"><span style="color:#A5D6FF">        "subtract"</span><span style="color:#E6EDF3">: a </span><span style="color:#FF7B72">-</span><span style="color:#E6EDF3"> b,</span></span>
<span class="line"><span style="color:#A5D6FF">        "multiply"</span><span style="color:#E6EDF3">: a </span><span style="color:#FF7B72">*</span><span style="color:#E6EDF3"> b,</span></span>
<span class="line"><span style="color:#A5D6FF">        "divide"</span><span style="color:#E6EDF3">: a </span><span style="color:#FF7B72">/</span><span style="color:#E6EDF3"> b </span><span style="color:#FF7B72">if</span><span style="color:#E6EDF3"> b </span><span style="color:#FF7B72">!=</span><span style="color:#79C0FF"> 0</span><span style="color:#FF7B72"> else</span><span style="color:#79C0FF"> float</span><span style="color:#E6EDF3">(</span><span style="color:#A5D6FF">'inf'</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">    }</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> operations.get(operation, </span><span style="color:#79C0FF">0</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">structured_calc </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> StructuredTool.from_function(</span></span>
<span class="line"><span style="color:#FFA657">    func</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">calculator_func,</span></span>
<span class="line"><span style="color:#FFA657">    name</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Calculator"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Performs basic arithmetic"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    args_schema</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">CalculatorInput</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Tool from API</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> requests</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> weather_tool</span><span style="color:#E6EDF3">(city: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#A5D6FF">    """Get current weather for a city"""</span></span>
<span class="line"><span style="color:#8B949E">    # Replace with actual API call</span></span>
<span class="line"><span style="color:#E6EDF3">    response </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> requests.get(</span></span>
<span class="line"><span style="color:#FF7B72">        f</span><span style="color:#A5D6FF">"https://api.weather.com/v1/current?city=</span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">city</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span></span>
<span class="line"><span style="color:#E6EDF3">    )</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> response.json()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">weather </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Tool.from_function(</span></span>
<span class="line"><span style="color:#FFA657">    func</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">weather_tool,</span></span>
<span class="line"><span style="color:#FFA657">    name</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Weather"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Gets current weather for a city"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-agent-types-and-usage">2. Agent Types and Usage</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.agents </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    create_openai_functions_agent,</span></span>
<span class="line"><span style="color:#E6EDF3">    create_react_agent,</span></span>
<span class="line"><span style="color:#E6EDF3">    AgentExecutor,</span></span>
<span class="line"><span style="color:#E6EDF3">    Tool</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.agents </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> load_tools</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Load built-in tools</span></span>
<span class="line"><span style="color:#E6EDF3">tools </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> load_tools(</span></span>
<span class="line"><span style="color:#E6EDF3">    [</span><span style="color:#A5D6FF">"serpapi"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"llm-math"</span><span style="color:#E6EDF3">],</span></span>
<span class="line"><span style="color:#FFA657">    llm</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">llm</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Custom tools</span></span>
<span class="line"><span style="color:#E6EDF3">custom_tools </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> [search_tool, calculator, weather]</span></span>
<span class="line"><span style="color:#E6EDF3">all_tools </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> tools </span><span style="color:#FF7B72">+</span><span style="color:#E6EDF3"> custom_tools</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># OpenAI Functions Agent</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> hub</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> hub.pull(</span><span style="color:#A5D6FF">"hwchase17/openai-functions-agent"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">agent </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> create_openai_functions_agent(llm, all_tools, prompt)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">agent_executor </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> AgentExecutor(</span></span>
<span class="line"><span style="color:#FFA657">    agent</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">agent,</span></span>
<span class="line"><span style="color:#FFA657">    tools</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">all_tools,</span></span>
<span class="line"><span style="color:#FFA657">    verbose</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    max_iterations</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">10</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    handle_parsing_errors</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Run agent</span></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> agent_executor.invoke({</span></span>
<span class="line"><span style="color:#A5D6FF">    "input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"What's the weather in Paris and what's 25 + 37?"</span></span>
<span class="line"><span style="color:#E6EDF3">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># ReAct Agent (Reasoning + Acting)</span></span>
<span class="line"><span style="color:#E6EDF3">react_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> hub.pull(</span><span style="color:#A5D6FF">"hwchase17/react"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">react_agent </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> create_react_agent(llm, all_tools, react_prompt)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">react_executor </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> AgentExecutor(</span></span>
<span class="line"><span style="color:#FFA657">    agent</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">react_agent,</span></span>
<span class="line"><span style="color:#FFA657">    tools</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">all_tools,</span></span>
<span class="line"><span style="color:#FFA657">    verbose</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Conversational Agent with memory</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.memory </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ConversationBufferMemory</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">memory </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ConversationBufferMemory(</span></span>
<span class="line"><span style="color:#FFA657">    memory_key</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"chat_history"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    return_messages</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">conversational_agent </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> AgentExecutor(</span></span>
<span class="line"><span style="color:#FFA657">    agent</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">agent,</span></span>
<span class="line"><span style="color:#FFA657">    tools</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">all_tools,</span></span>
<span class="line"><span style="color:#FFA657">    memory</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">memory,</span></span>
<span class="line"><span style="color:#FFA657">    verbose</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Multi-step agent execution</span></span>
<span class="line"><span style="color:#E6EDF3">response1 </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> conversational_agent.invoke({</span></span>
<span class="line"><span style="color:#A5D6FF">    "input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"My name is John"</span></span>
<span class="line"><span style="color:#E6EDF3">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">response2 </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> conversational_agent.invoke({</span></span>
<span class="line"><span style="color:#A5D6FF">    "input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"What's my name?"</span></span>
<span class="line"><span style="color:#E6EDF3">})  </span><span style="color:#8B949E"># Agent remembers: "John"</span></span>
<span class="line"></span></code></pre>
<h3 id="3-custom-agent-implementation">3. Custom Agent Implementation</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.agents </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> BaseSingleActionAgent</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.schema </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> AgentAction, AgentFinish</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> CustomAgent</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">BaseSingleActionAgent</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#E6EDF3">    tools: list[Tool]</span></span>
<span class="line"><span style="color:#E6EDF3">    llm: ChatOpenAI</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#D2A8FF">    @</span><span style="color:#79C0FF">property</span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> input_keys</span><span style="color:#E6EDF3">(self):</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#E6EDF3"> [</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> plan</span><span style="color:#E6EDF3">(</span></span>
<span class="line"><span style="color:#E6EDF3">        self,</span></span>
<span class="line"><span style="color:#E6EDF3">        intermediate_steps: </span><span style="color:#79C0FF">list</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FF7B72">        **</span><span style="color:#E6EDF3">kwargs</span></span>
<span class="line"><span style="color:#E6EDF3">    ) -> AgentAction </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> AgentFinish:</span></span>
<span class="line"><span style="color:#8B949E">        # Custom planning logic</span></span>
<span class="line"><span style="color:#E6EDF3">        user_input </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> kwargs[</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">        </span></span>
<span class="line"><span style="color:#8B949E">        # Decide which tool to use</span></span>
<span class="line"><span style="color:#FF7B72">        if</span><span style="color:#A5D6FF"> "weather"</span><span style="color:#FF7B72"> in</span><span style="color:#E6EDF3"> user_input.lower():</span></span>
<span class="line"><span style="color:#FF7B72">            return</span><span style="color:#E6EDF3"> AgentAction(</span></span>
<span class="line"><span style="color:#FFA657">                tool</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Weather"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">                tool_input</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">user_input,</span></span>
<span class="line"><span style="color:#FFA657">                log</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Using weather tool"</span></span>
<span class="line"><span style="color:#E6EDF3">            )</span></span>
<span class="line"><span style="color:#FF7B72">        elif</span><span style="color:#A5D6FF"> "calculate"</span><span style="color:#FF7B72"> in</span><span style="color:#E6EDF3"> user_input.lower():</span></span>
<span class="line"><span style="color:#FF7B72">            return</span><span style="color:#E6EDF3"> AgentAction(</span></span>
<span class="line"><span style="color:#FFA657">                tool</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Calculator"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">                tool_input</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">user_input,</span></span>
<span class="line"><span style="color:#FFA657">                log</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Using calculator"</span></span>
<span class="line"><span style="color:#E6EDF3">            )</span></span>
<span class="line"><span style="color:#FF7B72">        else</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#FF7B72">            return</span><span style="color:#E6EDF3"> AgentFinish(</span></span>
<span class="line"><span style="color:#FFA657">                return_values</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"output"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"I don't know how to help with that"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#FFA657">                log</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"No suitable tool found"</span></span>
<span class="line"><span style="color:#E6EDF3">            )</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> aplan</span><span style="color:#E6EDF3">(</span></span>
<span class="line"><span style="color:#E6EDF3">        self,</span></span>
<span class="line"><span style="color:#E6EDF3">        intermediate_steps: </span><span style="color:#79C0FF">list</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FF7B72">        **</span><span style="color:#E6EDF3">kwargs</span></span>
<span class="line"><span style="color:#E6EDF3">    ) -> AgentAction </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> AgentFinish:</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#79C0FF"> self</span><span style="color:#E6EDF3">.plan(intermediate_steps, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Use custom agent</span></span>
<span class="line"><span style="color:#E6EDF3">custom_agent </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> CustomAgent(</span><span style="color:#FFA657">tools</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">custom_tools, </span><span style="color:#FFA657">llm</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">llm)</span></span>
<span class="line"><span style="color:#E6EDF3">custom_executor </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> AgentExecutor(</span><span style="color:#FFA657">agent</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">custom_agent, </span><span style="color:#FFA657">tools</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">custom_tools)</span></span>
<span class="line"></span></code></pre>
<h2 id="memory-systems-memory">Memory Systems {#memory}</h2>
<h3 id="1-memory-types">1. Memory Types</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.memory </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    ConversationBufferMemory,</span></span>
<span class="line"><span style="color:#E6EDF3">    ConversationBufferWindowMemory,</span></span>
<span class="line"><span style="color:#E6EDF3">    ConversationSummaryMemory,</span></span>
<span class="line"><span style="color:#E6EDF3">    ConversationKGMemory,</span></span>
<span class="line"><span style="color:#E6EDF3">    VectorStoreRetrieverMemory</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Buffer memory - stores all messages</span></span>
<span class="line"><span style="color:#E6EDF3">buffer_memory </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ConversationBufferMemory()</span></span>
<span class="line"><span style="color:#E6EDF3">buffer_memory.save_context(</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Hi, I'm John"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"output"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Hello John! How can I help you?"</span><span style="color:#E6EDF3">}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Window memory - keeps last K messages</span></span>
<span class="line"><span style="color:#E6EDF3">window_memory </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ConversationBufferWindowMemory(</span><span style="color:#FFA657">k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">5</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Summary memory - summarizes old conversations</span></span>
<span class="line"><span style="color:#E6EDF3">summary_memory </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ConversationSummaryMemory(</span><span style="color:#FFA657">llm</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">llm)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Knowledge graph memory - extracts entities and relationships</span></span>
<span class="line"><span style="color:#E6EDF3">kg_memory </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ConversationKGMemory(</span><span style="color:#FFA657">llm</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">llm)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Vector store memory - uses similarity search</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Chroma</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">embeddings </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> OpenAIEmbeddings()</span></span>
<span class="line"><span style="color:#E6EDF3">vector_store </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Chroma(</span><span style="color:#FFA657">embedding_function</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">embeddings)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">vector_memory </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> VectorStoreRetrieverMemory(</span></span>
<span class="line"><span style="color:#FFA657">    retriever</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">vector_store.as_retriever(</span><span style="color:#FFA657">search_kwargs</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"k"</span><span style="color:#E6EDF3">: </span><span style="color:#79C0FF">3</span><span style="color:#E6EDF3">})</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Add memories</span></span>
<span class="line"><span style="color:#E6EDF3">vector_memory.save_context(</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"I love programming"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"output"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"That's great! What languages?"</span><span style="color:#E6EDF3">}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-memory-in-chains">2. Memory in Chains</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Chain with memory</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.chains </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ConversationChain</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">conversation </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ConversationChain(</span></span>
<span class="line"><span style="color:#FFA657">    llm</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">llm,</span></span>
<span class="line"><span style="color:#FFA657">    memory</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">buffer_memory,</span></span>
<span class="line"><span style="color:#FFA657">    verbose</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Conversation maintains context</span></span>
<span class="line"><span style="color:#E6EDF3">response1 </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> conversation.predict(</span><span style="color:#FFA657">input</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Hi, I'm Alice"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">response2 </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> conversation.predict(</span><span style="color:#FFA657">input</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"What's my name?"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#8B949E"># Response: "Your name is Alice"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Custom memory implementation</span></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> CustomMemory</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">ConversationBufferMemory</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> save_context</span><span style="color:#E6EDF3">(self, inputs: </span><span style="color:#79C0FF">dict</span><span style="color:#E6EDF3">, outputs: </span><span style="color:#79C0FF">dict</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#8B949E">        # Custom logic before saving</span></span>
<span class="line"><span style="color:#8B949E">        # E.g., filter sensitive information</span></span>
<span class="line"><span style="color:#E6EDF3">        filtered_inputs </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> self</span><span style="color:#E6EDF3">._filter_sensitive(inputs)</span></span>
<span class="line"><span style="color:#79C0FF">        super</span><span style="color:#E6EDF3">().save_context(filtered_inputs, outputs)</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> _filter_sensitive</span><span style="color:#E6EDF3">(self, inputs: </span><span style="color:#79C0FF">dict</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">dict</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#8B949E">        # Remove credit card numbers, etc.</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#E6EDF3"> inputs</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Memory with multiple stores</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.memory </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> CombinedMemory</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Short-term memory</span></span>
<span class="line"><span style="color:#E6EDF3">short_term </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ConversationBufferWindowMemory(</span><span style="color:#FFA657">k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">5</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Long-term memory  </span></span>
<span class="line"><span style="color:#E6EDF3">long_term </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> VectorStoreRetrieverMemory(</span></span>
<span class="line"><span style="color:#FFA657">    retriever</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">vector_store.as_retriever()</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Combine memories</span></span>
<span class="line"><span style="color:#E6EDF3">combined_memory </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> CombinedMemory(</span><span style="color:#FFA657">memories</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[short_term, long_term])</span></span>
<span class="line"></span></code></pre>
<h2 id="rag-retrieval-augmented-generation-rag">RAG (Retrieval Augmented Generation) {#rag}</h2>
<h3 id="1-basic-rag-implementation">1. Basic RAG Implementation</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.document_loaders </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    TextLoader,</span></span>
<span class="line"><span style="color:#E6EDF3">    PyPDFLoader,</span></span>
<span class="line"><span style="color:#E6EDF3">    WebBaseLoader</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.text_splitter </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> RecursiveCharacterTextSplitter</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Chroma</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.output_parsers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> StrOutputParser</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.runnables </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> RunnablePassthrough</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Load documents</span></span>
<span class="line"><span style="color:#E6EDF3">loader </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> PyPDFLoader(</span><span style="color:#A5D6FF">"document.pdf"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">documents </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> loader.load()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Split documents</span></span>
<span class="line"><span style="color:#E6EDF3">text_splitter </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> RecursiveCharacterTextSplitter(</span></span>
<span class="line"><span style="color:#FFA657">    chunk_size</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">1000</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    chunk_overlap</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">200</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    length_function</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">len</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">splits </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> text_splitter.split_documents(documents)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Create embeddings and vector store</span></span>
<span class="line"><span style="color:#E6EDF3">embeddings </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> OpenAIEmbeddings()</span></span>
<span class="line"><span style="color:#E6EDF3">vectorstore </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Chroma.from_documents(</span></span>
<span class="line"><span style="color:#FFA657">    documents</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">splits,</span></span>
<span class="line"><span style="color:#FFA657">    embedding</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">embeddings</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Create retriever</span></span>
<span class="line"><span style="color:#E6EDF3">retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> vectorstore.as_retriever(</span></span>
<span class="line"><span style="color:#FFA657">    search_type</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"similarity"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    search_kwargs</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"k"</span><span style="color:#E6EDF3">: </span><span style="color:#79C0FF">3</span><span style="color:#E6EDF3">}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># RAG prompt</span></span>
<span class="line"><span style="color:#E6EDF3">rag_prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span><span style="color:#A5D6FF">"""</span></span>
<span class="line"><span style="color:#A5D6FF">Answer the question based only on the following context:</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">{context}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Question: </span><span style="color:#FF7B72">{question}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A5D6FF">Answer:</span></span>
<span class="line"><span style="color:#A5D6FF">"""</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Format documents</span></span>
<span class="line"><span style="color:#FF7B72">def</span><span style="color:#D2A8FF"> format_docs</span><span style="color:#E6EDF3">(docs):</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#A5D6FF"> "</span><span style="color:#FF7B72">\n\n</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">.join(doc.page_content </span><span style="color:#FF7B72">for</span><span style="color:#E6EDF3"> doc </span><span style="color:#FF7B72">in</span><span style="color:#E6EDF3"> docs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Create RAG chain</span></span>
<span class="line"><span style="color:#E6EDF3">rag_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span></span>
<span class="line"><span style="color:#A5D6FF">        "context"</span><span style="color:#E6EDF3">: retriever </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> format_docs,</span></span>
<span class="line"><span style="color:#A5D6FF">        "question"</span><span style="color:#E6EDF3">: RunnablePassthrough()</span></span>
<span class="line"><span style="color:#E6EDF3">    }</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> rag_prompt</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> llm</span></span>
<span class="line"><span style="color:#FF7B72">    |</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Query</span></span>
<span class="line"><span style="color:#E6EDF3">answer </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> rag_chain.invoke(</span><span style="color:#A5D6FF">"What is the main topic of the document?"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-advanced-rag-techniques">2. Advanced RAG Techniques</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Multi-query retrieval</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.retrievers.multi_query </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> MultiQueryRetriever</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">multi_query_retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> MultiQueryRetriever.from_llm(</span></span>
<span class="line"><span style="color:#FFA657">    retriever</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">vectorstore.as_retriever(),</span></span>
<span class="line"><span style="color:#FFA657">    llm</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">llm</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Generates multiple queries and retrieves for each</span></span>
<span class="line"><span style="color:#E6EDF3">results </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> multi_query_retriever.get_relevant_documents(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Tell me about climate change"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Contextual compression</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.retrievers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ContextualCompressionRetriever</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.retrievers.document_compressors </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> LLMChainExtractor</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">compressor </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> LLMChainExtractor.from_llm(llm)</span></span>
<span class="line"><span style="color:#E6EDF3">compression_retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ContextualCompressionRetriever(</span></span>
<span class="line"><span style="color:#FFA657">    base_compressor</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">compressor,</span></span>
<span class="line"><span style="color:#FFA657">    base_retriever</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">vectorstore.as_retriever()</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Returns only relevant parts of documents</span></span>
<span class="line"><span style="color:#E6EDF3">compressed_docs </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> compression_retriever.get_relevant_documents(</span></span>
<span class="line"><span style="color:#A5D6FF">    "What is Python?"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Parent document retrieval</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.retrievers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ParentDocumentRetriever</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.storage </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> InMemoryStore</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Store for parent documents</span></span>
<span class="line"><span style="color:#E6EDF3">store </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> InMemoryStore()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Small chunks for retrieval</span></span>
<span class="line"><span style="color:#E6EDF3">child_splitter </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> RecursiveCharacterTextSplitter(</span><span style="color:#FFA657">chunk_size</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">400</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Larger chunks for context</span></span>
<span class="line"><span style="color:#E6EDF3">parent_splitter </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> RecursiveCharacterTextSplitter(</span><span style="color:#FFA657">chunk_size</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">2000</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">parent_retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ParentDocumentRetriever(</span></span>
<span class="line"><span style="color:#FFA657">    vectorstore</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">vectorstore,</span></span>
<span class="line"><span style="color:#FFA657">    docstore</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">store,</span></span>
<span class="line"><span style="color:#FFA657">    child_splitter</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">child_splitter,</span></span>
<span class="line"><span style="color:#FFA657">    parent_splitter</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">parent_splitter,</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Hybrid search (keyword + semantic)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.retrievers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> BM25Retriever, EnsembleRetriever</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Keyword-based retriever</span></span>
<span class="line"><span style="color:#E6EDF3">bm25_retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> BM25Retriever.from_documents(splits)</span></span>
<span class="line"><span style="color:#E6EDF3">bm25_retriever.k </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> 3</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Semantic retriever</span></span>
<span class="line"><span style="color:#E6EDF3">semantic_retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> vectorstore.as_retriever(</span><span style="color:#FFA657">search_kwargs</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"k"</span><span style="color:#E6EDF3">: </span><span style="color:#79C0FF">3</span><span style="color:#E6EDF3">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Combine both</span></span>
<span class="line"><span style="color:#E6EDF3">ensemble_retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> EnsembleRetriever(</span></span>
<span class="line"><span style="color:#FFA657">    retrievers</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[bm25_retriever, semantic_retriever],</span></span>
<span class="line"><span style="color:#FFA657">    weights</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span><span style="color:#79C0FF">0.5</span><span style="color:#E6EDF3">, </span><span style="color:#79C0FF">0.5</span><span style="color:#E6EDF3">]</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Self-query retriever with metadata</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.retrievers.self_query.base </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> SelfQueryRetriever</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.chains.query_constructor.base </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> AttributeInfo</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">metadata_field_info </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> [</span></span>
<span class="line"><span style="color:#E6EDF3">    AttributeInfo(</span></span>
<span class="line"><span style="color:#FFA657">        name</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"category"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">        description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"The category of the document"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">        type</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"string"</span></span>
<span class="line"><span style="color:#E6EDF3">    ),</span></span>
<span class="line"><span style="color:#E6EDF3">    AttributeInfo(</span></span>
<span class="line"><span style="color:#FFA657">        name</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"year"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">        description</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"The year the document was written"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">        type</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"integer"</span></span>
<span class="line"><span style="color:#E6EDF3">    ),</span></span>
<span class="line"><span style="color:#E6EDF3">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">document_content_description </span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF"> "Technical documentation"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">self_query_retriever </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> SelfQueryRetriever.from_llm(</span></span>
<span class="line"><span style="color:#E6EDF3">    llm,</span></span>
<span class="line"><span style="color:#E6EDF3">    vectorstore,</span></span>
<span class="line"><span style="color:#E6EDF3">    document_content_description,</span></span>
<span class="line"><span style="color:#E6EDF3">    metadata_field_info,</span></span>
<span class="line"><span style="color:#FFA657">    verbose</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">True</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Query with metadata filter</span></span>
<span class="line"><span style="color:#E6EDF3">docs </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> self_query_retriever.get_relevant_documents(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Documents about Python from 2023"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h2 id="vector-stores-vector-stores">Vector Stores {#vector-stores}</h2>
<h3 id="1-vector-store-options">1. Vector Store Options</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Chroma (local)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Chroma</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">chroma_db </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Chroma.from_documents(</span></span>
<span class="line"><span style="color:#FFA657">    documents</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">splits,</span></span>
<span class="line"><span style="color:#FFA657">    embedding</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">embeddings,</span></span>
<span class="line"><span style="color:#FFA657">    persist_directory</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"./chroma_db"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># FAISS (local, fast)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#79C0FF"> FAISS</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">faiss_db </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> FAISS</span><span style="color:#E6EDF3">.from_documents(splits, embeddings)</span></span>
<span class="line"><span style="color:#E6EDF3">faiss_db.save_local(</span><span style="color:#A5D6FF">"faiss_index"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Load existing index</span></span>
<span class="line"><span style="color:#E6EDF3">loaded_db </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> FAISS</span><span style="color:#E6EDF3">.load_local(</span><span style="color:#A5D6FF">"faiss_index"</span><span style="color:#E6EDF3">, embeddings)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Pinecone (cloud)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Pinecone</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> pinecone</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">pinecone.init(</span><span style="color:#FFA657">api_key</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"your-key"</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">environment</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"us-west1-gcp"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">pinecone_db </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Pinecone.from_documents(</span></span>
<span class="line"><span style="color:#E6EDF3">    splits,</span></span>
<span class="line"><span style="color:#E6EDF3">    embeddings,</span></span>
<span class="line"><span style="color:#FFA657">    index_name</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"my-index"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Weaviate (cloud/self-hosted)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Weaviate</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> weaviate</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">client </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> weaviate.Client(</span><span style="color:#FFA657">url</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"http://localhost:8080"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">weaviate_db </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Weaviate.from_documents(</span></span>
<span class="line"><span style="color:#E6EDF3">    splits,</span></span>
<span class="line"><span style="color:#E6EDF3">    embeddings,</span></span>
<span class="line"><span style="color:#FFA657">    client</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">client,</span></span>
<span class="line"><span style="color:#FFA657">    by_text</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">False</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Qdrant (cloud/self-hosted)</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_community.vectorstores </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Qdrant</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">qdrant_db </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Qdrant.from_documents(</span></span>
<span class="line"><span style="color:#E6EDF3">    splits,</span></span>
<span class="line"><span style="color:#E6EDF3">    embeddings,</span></span>
<span class="line"><span style="color:#FFA657">    url</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"http://localhost:6333"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    collection_name</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"my_documents"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-vector-store-operations">2. Vector Store Operations</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#8B949E"># Add documents</span></span>
<span class="line"><span style="color:#E6EDF3">new_docs </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> text_splitter.create_documents([</span></span>
<span class="line"><span style="color:#A5D6FF">    "New document content"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">    "Another document"</span></span>
<span class="line"><span style="color:#E6EDF3">])</span></span>
<span class="line"><span style="color:#E6EDF3">vectorstore.add_documents(new_docs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Similarity search</span></span>
<span class="line"><span style="color:#E6EDF3">results </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> vectorstore.similarity_search(</span></span>
<span class="line"><span style="color:#A5D6FF">    "What is machine learning?"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">5</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Similarity search with scores</span></span>
<span class="line"><span style="color:#E6EDF3">results_with_scores </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> vectorstore.similarity_search_with_score(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Python programming"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">3</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">for</span><span style="color:#E6EDF3"> doc, score </span><span style="color:#FF7B72">in</span><span style="color:#E6EDF3"> results_with_scores:</span></span>
<span class="line"><span style="color:#79C0FF">    print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Score: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">score</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">    print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Content: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">doc.page_content[:</span><span style="color:#79C0FF">100</span><span style="color:#E6EDF3">]</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">..."</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># MMR (Maximal Marginal Relevance) search</span></span>
<span class="line"><span style="color:#8B949E"># Returns diverse results</span></span>
<span class="line"><span style="color:#E6EDF3">mmr_results </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> vectorstore.max_marginal_relevance_search(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Python"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">5</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    fetch_k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">20</span><span style="color:#8B949E">  # Fetch more, return diverse subset</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Metadata filtering</span></span>
<span class="line"><span style="color:#E6EDF3">filtered_results </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> vectorstore.similarity_search(</span></span>
<span class="line"><span style="color:#A5D6FF">    "Python tutorial"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    k</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">5</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    filter</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"category"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"programming"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"year"</span><span style="color:#E6EDF3">: </span><span style="color:#79C0FF">2023</span><span style="color:#E6EDF3">}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Delete documents</span></span>
<span class="line"><span style="color:#E6EDF3">vectorstore.delete(</span><span style="color:#FFA657">ids</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span><span style="color:#A5D6FF">"doc_id_1"</span><span style="color:#E6EDF3">, </span><span style="color:#A5D6FF">"doc_id_2"</span><span style="color:#E6EDF3">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Update documents</span></span>
<span class="line"><span style="color:#E6EDF3">vectorstore.update_document(</span></span>
<span class="line"><span style="color:#FFA657">    document_id</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"doc_id"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    document</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">new_document</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h2 id="evaluation-and-monitoring-evaluation">Evaluation and Monitoring {#evaluation}</h2>
<h3 id="1-evaluating-rag-systems">1. Evaluating RAG Systems</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.evaluation </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    load_evaluator,</span></span>
<span class="line"><span style="color:#E6EDF3">    EvaluatorType</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Question-Answer evaluation</span></span>
<span class="line"><span style="color:#E6EDF3">qa_evaluator </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> load_evaluator(</span><span style="color:#A5D6FF">"qa"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">eval_result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> qa_evaluator.evaluate_strings(</span></span>
<span class="line"><span style="color:#FFA657">    prediction</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Paris is the capital of France"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    input</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"What is the capital of France?"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    reference</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"Paris"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Criteria-based evaluation</span></span>
<span class="line"><span style="color:#E6EDF3">criteria_evaluator </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> load_evaluator(</span></span>
<span class="line"><span style="color:#A5D6FF">    "criteria"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    criteria</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"conciseness"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">eval_result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> criteria_evaluator.evaluate_strings(</span></span>
<span class="line"><span style="color:#FFA657">    prediction</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"The answer is very long and verbose..."</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    input</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"What is 2+2?"</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Custom evaluation</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.evaluation </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> StringEvaluator</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> CustomEvaluator</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">StringEvaluator</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> _evaluate_strings</span><span style="color:#E6EDF3">(</span></span>
<span class="line"><span style="color:#E6EDF3">        self,</span></span>
<span class="line"><span style="color:#E6EDF3">        prediction: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#E6EDF3">        reference: </span><span style="color:#79C0FF">str</span><span style="color:#FF7B72"> =</span><span style="color:#79C0FF"> None</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#E6EDF3">        input: </span><span style="color:#79C0FF">str</span><span style="color:#FF7B72"> =</span><span style="color:#79C0FF"> None</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FF7B72">        **</span><span style="color:#E6EDF3">kwargs</span></span>
<span class="line"><span style="color:#E6EDF3">    ) -> </span><span style="color:#79C0FF">dict</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#8B949E">        # Custom evaluation logic</span></span>
<span class="line"><span style="color:#E6EDF3">        score </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> len</span><span style="color:#E6EDF3">(prediction) </span><span style="color:#FF7B72">&#x3C;</span><span style="color:#79C0FF"> 100</span><span style="color:#8B949E">  # Example: brevity</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#E6EDF3"> {</span></span>
<span class="line"><span style="color:#A5D6FF">            "score"</span><span style="color:#E6EDF3">: </span><span style="color:#79C0FF">int</span><span style="color:#E6EDF3">(score),</span></span>
<span class="line"><span style="color:#A5D6FF">            "reasoning"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Response is concise"</span><span style="color:#FF7B72"> if</span><span style="color:#E6EDF3"> score </span><span style="color:#FF7B72">else</span><span style="color:#A5D6FF"> "Too long"</span></span>
<span class="line"><span style="color:#E6EDF3">        }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># RAG evaluation metrics</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> ragas </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> evaluate</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> ragas.metrics </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> (</span></span>
<span class="line"><span style="color:#E6EDF3">    faithfulness,</span></span>
<span class="line"><span style="color:#E6EDF3">    answer_relevancy,</span></span>
<span class="line"><span style="color:#E6EDF3">    context_precision,</span></span>
<span class="line"><span style="color:#E6EDF3">    context_recall</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Prepare evaluation dataset</span></span>
<span class="line"><span style="color:#E6EDF3">eval_dataset </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> {</span></span>
<span class="line"><span style="color:#A5D6FF">    "question"</span><span style="color:#E6EDF3">: [</span><span style="color:#A5D6FF">"What is Python?"</span><span style="color:#E6EDF3">],</span></span>
<span class="line"><span style="color:#A5D6FF">    "answer"</span><span style="color:#E6EDF3">: [</span><span style="color:#A5D6FF">"Python is a programming language"</span><span style="color:#E6EDF3">],</span></span>
<span class="line"><span style="color:#A5D6FF">    "contexts"</span><span style="color:#E6EDF3">: [[</span><span style="color:#A5D6FF">"Python is a high-level programming language..."</span><span style="color:#E6EDF3">]],</span></span>
<span class="line"><span style="color:#A5D6FF">    "ground_truths"</span><span style="color:#E6EDF3">: [[</span><span style="color:#A5D6FF">"Python is a programming language"</span><span style="color:#E6EDF3">]]</span></span>
<span class="line"><span style="color:#E6EDF3">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Evaluate</span></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> evaluate(</span></span>
<span class="line"><span style="color:#E6EDF3">    eval_dataset,</span></span>
<span class="line"><span style="color:#FFA657">    metrics</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">[</span></span>
<span class="line"><span style="color:#E6EDF3">        faithfulness,</span></span>
<span class="line"><span style="color:#E6EDF3">        answer_relevancy,</span></span>
<span class="line"><span style="color:#E6EDF3">        context_precision,</span></span>
<span class="line"><span style="color:#E6EDF3">        context_recall</span></span>
<span class="line"><span style="color:#E6EDF3">    ]</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79C0FF">print</span><span style="color:#E6EDF3">(result)</span></span>
<span class="line"></span></code></pre>
<h3 id="2-monitoring-and-logging">2. Monitoring and Logging</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.callbacks </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> StdOutCallbackHandler</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.callbacks.tracers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> LangChainTracer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Stdout logging</span></span>
<span class="line"><span style="color:#E6EDF3">handler </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> StdOutCallbackHandler()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain.invoke(</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Hello"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#FFA657">    config</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"callbacks"</span><span style="color:#E6EDF3">: [handler]}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># LangSmith tracing</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> os</span></span>
<span class="line"><span style="color:#E6EDF3">os.environ[</span><span style="color:#A5D6FF">"LANGCHAIN_TRACING_V2"</span><span style="color:#E6EDF3">] </span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF"> "true"</span></span>
<span class="line"><span style="color:#E6EDF3">os.environ[</span><span style="color:#A5D6FF">"LANGCHAIN_API_KEY"</span><span style="color:#E6EDF3">] </span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF"> "your-key"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">tracer </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> LangChainTracer()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain.invoke(</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Hello"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#FFA657">    config</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"callbacks"</span><span style="color:#E6EDF3">: [tracer]}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Custom callback</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.callbacks.base </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> BaseCallbackHandler</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> CustomCallback</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">BaseCallbackHandler</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> on_llm_start</span><span style="color:#E6EDF3">(self, serialized, prompts, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs):</span></span>
<span class="line"><span style="color:#79C0FF">        print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"LLM started with prompts: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">prompts</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> on_llm_end</span><span style="color:#E6EDF3">(self, response, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs):</span></span>
<span class="line"><span style="color:#79C0FF">        print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"LLM ended with response: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">response</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> on_llm_error</span><span style="color:#E6EDF3">(self, error, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs):</span></span>
<span class="line"><span style="color:#79C0FF">        print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"LLM error: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">error</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> on_chain_start</span><span style="color:#E6EDF3">(self, serialized, inputs, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs):</span></span>
<span class="line"><span style="color:#79C0FF">        print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Chain started with inputs: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">inputs</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> on_chain_end</span><span style="color:#E6EDF3">(self, outputs, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs):</span></span>
<span class="line"><span style="color:#79C0FF">        print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Chain ended with outputs: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">outputs</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">custom_callback </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> CustomCallback()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Token counting callback</span></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> TokenCountCallback</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">BaseCallbackHandler</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#79C0FF"> __init__</span><span style="color:#E6EDF3">(self):</span></span>
<span class="line"><span style="color:#79C0FF">        self</span><span style="color:#E6EDF3">.total_tokens </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> 0</span></span>
<span class="line"><span style="color:#79C0FF">        self</span><span style="color:#E6EDF3">.prompt_tokens </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> 0</span></span>
<span class="line"><span style="color:#79C0FF">        self</span><span style="color:#E6EDF3">.completion_tokens </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> 0</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#D2A8FF"> on_llm_end</span><span style="color:#E6EDF3">(self, response, </span><span style="color:#FF7B72">**</span><span style="color:#E6EDF3">kwargs):</span></span>
<span class="line"><span style="color:#FF7B72">        if</span><span style="color:#79C0FF"> hasattr</span><span style="color:#E6EDF3">(response, </span><span style="color:#A5D6FF">"llm_output"</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#E6EDF3">            token_usage </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> response.llm_output.get(</span><span style="color:#A5D6FF">"token_usage"</span><span style="color:#E6EDF3">, {})</span></span>
<span class="line"><span style="color:#79C0FF">            self</span><span style="color:#E6EDF3">.total_tokens </span><span style="color:#FF7B72">+=</span><span style="color:#E6EDF3"> token_usage.get(</span><span style="color:#A5D6FF">"total_tokens"</span><span style="color:#E6EDF3">, </span><span style="color:#79C0FF">0</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">            self</span><span style="color:#E6EDF3">.prompt_tokens </span><span style="color:#FF7B72">+=</span><span style="color:#E6EDF3"> token_usage.get(</span><span style="color:#A5D6FF">"prompt_tokens"</span><span style="color:#E6EDF3">, </span><span style="color:#79C0FF">0</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#79C0FF">            self</span><span style="color:#E6EDF3">.completion_tokens </span><span style="color:#FF7B72">+=</span><span style="color:#E6EDF3"> token_usage.get(</span><span style="color:#A5D6FF">"completion_tokens"</span><span style="color:#E6EDF3">, </span><span style="color:#79C0FF">0</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">token_counter </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> TokenCountCallback()</span></span>
<span class="line"><span style="color:#E6EDF3">result </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain.invoke(</span></span>
<span class="line"><span style="color:#E6EDF3">    {</span><span style="color:#A5D6FF">"input"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"Hello"</span><span style="color:#E6EDF3">},</span></span>
<span class="line"><span style="color:#FFA657">    config</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"callbacks"</span><span style="color:#E6EDF3">: [token_counter]}</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79C0FF">print</span><span style="color:#E6EDF3">(</span><span style="color:#FF7B72">f</span><span style="color:#A5D6FF">"Total tokens used: </span><span style="color:#FF7B72">{</span><span style="color:#E6EDF3">token_counter.total_tokens</span><span style="color:#FF7B72">}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h2 id="production-deployment-deployment">Production Deployment {#deployment}</h2>
<h3 id="1-fastapi-integration">1. FastAPI Integration</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> fastapi </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> FastAPI, HTTPException</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> pydantic </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> BaseModel</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_openai </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatOpenAI</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.prompts </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> ChatPromptTemplate</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.output_parsers </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> StrOutputParser</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">app </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> FastAPI(</span><span style="color:#FFA657">title</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"LangChain API"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Initialize LLM and chain</span></span>
<span class="line"><span style="color:#E6EDF3">llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatOpenAI(</span><span style="color:#FFA657">model</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"gpt-3.5-turbo"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">prompt </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatPromptTemplate.from_template(</span><span style="color:#A5D6FF">"Answer: </span><span style="color:#FF7B72">{question}</span><span style="color:#A5D6FF">"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> prompt </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> llm </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> StrOutputParser()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Request model</span></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> QuestionRequest</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">BaseModel</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#E6EDF3">    question: </span><span style="color:#79C0FF">str</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> AnswerResponse</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">BaseModel</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#E6EDF3">    answer: </span><span style="color:#79C0FF">str</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D2A8FF">@app.post</span><span style="color:#E6EDF3">(</span><span style="color:#A5D6FF">"/ask"</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">response_model</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">AnswerResponse)</span></span>
<span class="line"><span style="color:#FF7B72">async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> ask_question</span><span style="color:#E6EDF3">(request: QuestionRequest):</span></span>
<span class="line"><span style="color:#FF7B72">    try</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#E6EDF3">        answer </span><span style="color:#FF7B72">=</span><span style="color:#FF7B72"> await</span><span style="color:#E6EDF3"> chain.ainvoke({</span><span style="color:#A5D6FF">"question"</span><span style="color:#E6EDF3">: request.question})</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#E6EDF3"> AnswerResponse(</span><span style="color:#FFA657">answer</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">answer)</span></span>
<span class="line"><span style="color:#FF7B72">    except</span><span style="color:#79C0FF"> Exception</span><span style="color:#FF7B72"> as</span><span style="color:#E6EDF3"> e:</span></span>
<span class="line"><span style="color:#FF7B72">        raise</span><span style="color:#E6EDF3"> HTTPException(</span><span style="color:#FFA657">status_code</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">500</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">detail</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">(e))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># RAG endpoint</span></span>
<span class="line"><span style="color:#D2A8FF">@app.post</span><span style="color:#E6EDF3">(</span><span style="color:#A5D6FF">"/rag"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#FF7B72">async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> rag_query</span><span style="color:#E6EDF3">(request: QuestionRequest):</span></span>
<span class="line"><span style="color:#FF7B72">    try</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#8B949E">        # Use RAG chain</span></span>
<span class="line"><span style="color:#E6EDF3">        answer </span><span style="color:#FF7B72">=</span><span style="color:#FF7B72"> await</span><span style="color:#E6EDF3"> rag_chain.ainvoke(request.question)</span></span>
<span class="line"><span style="color:#FF7B72">        return</span><span style="color:#E6EDF3"> {</span><span style="color:#A5D6FF">"answer"</span><span style="color:#E6EDF3">: answer}</span></span>
<span class="line"><span style="color:#FF7B72">    except</span><span style="color:#79C0FF"> Exception</span><span style="color:#FF7B72"> as</span><span style="color:#E6EDF3"> e:</span></span>
<span class="line"><span style="color:#FF7B72">        raise</span><span style="color:#E6EDF3"> HTTPException(</span><span style="color:#FFA657">status_code</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">500</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">detail</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">(e))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Health check</span></span>
<span class="line"><span style="color:#D2A8FF">@app.get</span><span style="color:#E6EDF3">(</span><span style="color:#A5D6FF">"/health"</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#FF7B72">async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> health</span><span style="color:#E6EDF3">():</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> {</span><span style="color:#A5D6FF">"status"</span><span style="color:#E6EDF3">: </span><span style="color:#A5D6FF">"healthy"</span><span style="color:#E6EDF3">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Run with: uvicorn main:app --reload</span></span>
<span class="line"></span></code></pre>
<h3 id="2-caching-and-optimization">2. Caching and Optimization</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.cache </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> InMemoryCache, SQLiteCache</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.globals </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> set_llm_cache</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> langchain</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># In-memory cache</span></span>
<span class="line"><span style="color:#E6EDF3">set_llm_cache(InMemoryCache())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># SQLite cache (persistent)</span></span>
<span class="line"><span style="color:#E6EDF3">set_llm_cache(SQLiteCache(</span><span style="color:#FFA657">database_path</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">".langchain.db"</span><span style="color:#E6EDF3">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Redis cache</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.cache </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> RedisCache</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> redis</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">redis_client </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> redis.Redis(</span><span style="color:#FFA657">host</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">'localhost'</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">port</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">6379</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">set_llm_cache(RedisCache(redis_client))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Semantic cache</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain.cache </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> RedisSemanticCache</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">set_llm_cache(</span></span>
<span class="line"><span style="color:#E6EDF3">    RedisSemanticCache(</span></span>
<span class="line"><span style="color:#FFA657">        redis_url</span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF">"redis://localhost:6379"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">        embedding</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">OpenAIEmbeddings()</span></span>
<span class="line"><span style="color:#E6EDF3">    )</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Rate limiting</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> langchain_core.runnables </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> RateLimiter</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">rate_limiter </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> RateLimiter(</span></span>
<span class="line"><span style="color:#FFA657">    requests_per_second</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">10</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">    check_every_n_seconds</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">1</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">rate_limited_chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> rate_limiter </span><span style="color:#FF7B72">|</span><span style="color:#E6EDF3"> chain</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Batch processing</span></span>
<span class="line"><span style="color:#FF7B72">async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> process_batch</span><span style="color:#E6EDF3">(questions: list[</span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">]):</span></span>
<span class="line"><span style="color:#E6EDF3">    results </span><span style="color:#FF7B72">=</span><span style="color:#FF7B72"> await</span><span style="color:#E6EDF3"> chain.abatch(</span></span>
<span class="line"><span style="color:#E6EDF3">        [{</span><span style="color:#A5D6FF">"question"</span><span style="color:#E6EDF3">: q} </span><span style="color:#FF7B72">for</span><span style="color:#E6EDF3"> q </span><span style="color:#FF7B72">in</span><span style="color:#E6EDF3"> questions],</span></span>
<span class="line"><span style="color:#FFA657">        config</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">{</span><span style="color:#A5D6FF">"max_concurrency"</span><span style="color:#E6EDF3">: </span><span style="color:#79C0FF">5</span><span style="color:#E6EDF3">}</span></span>
<span class="line"><span style="color:#E6EDF3">    )</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#E6EDF3"> results</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Retry logic</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> tenacity </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> retry, stop_after_attempt, wait_exponential</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D2A8FF">@retry</span><span style="color:#E6EDF3">(</span></span>
<span class="line"><span style="color:#FFA657">    stop</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">stop_after_attempt(</span><span style="color:#79C0FF">3</span><span style="color:#E6EDF3">),</span></span>
<span class="line"><span style="color:#FFA657">    wait</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">wait_exponential(</span><span style="color:#FFA657">multiplier</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">1</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">min</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">4</span><span style="color:#E6EDF3">, </span><span style="color:#FFA657">max</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">10</span><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#FF7B72">async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> resilient_invoke</span><span style="color:#E6EDF3">(question: </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#FF7B72">    return</span><span style="color:#FF7B72"> await</span><span style="color:#E6EDF3"> chain.ainvoke({</span><span style="color:#A5D6FF">"question"</span><span style="color:#E6EDF3">: question})</span></span>
<span class="line"></span></code></pre>
<h3 id="3-production-best-practices">3. Production Best Practices</h3>
<pre class="astro-code github-dark-default" style="background-color:#0d1117;color:#e6edf3; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> structlog</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> prometheus_client </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> Counter, Histogram</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> time</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Structured logging</span></span>
<span class="line"><span style="color:#E6EDF3">logger </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> structlog.get_logger()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Metrics</span></span>
<span class="line"><span style="color:#E6EDF3">request_counter </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Counter(</span></span>
<span class="line"><span style="color:#A5D6FF">    'langchain_requests_total'</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">    'Total LangChain requests'</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"><span style="color:#E6EDF3">request_duration </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Histogram(</span></span>
<span class="line"><span style="color:#A5D6FF">    'langchain_request_duration_seconds'</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#A5D6FF">    'Request duration'</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Production-ready chain wrapper</span></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> ProductionChain</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#FF7B72">    def</span><span style="color:#79C0FF"> __init__</span><span style="color:#E6EDF3">(self, chain):</span></span>
<span class="line"><span style="color:#79C0FF">        self</span><span style="color:#E6EDF3">.chain </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> chain</span></span>
<span class="line"><span style="color:#79C0FF">        self</span><span style="color:#E6EDF3">.logger </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> structlog.get_logger()</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    async</span><span style="color:#FF7B72"> def</span><span style="color:#D2A8FF"> invoke</span><span style="color:#E6EDF3">(self, input_data: </span><span style="color:#79C0FF">dict</span><span style="color:#E6EDF3">) -> </span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#E6EDF3">        request_id </span><span style="color:#FF7B72">=</span><span style="color:#79C0FF"> str</span><span style="color:#E6EDF3">(uuid.uuid4())</span></span>
<span class="line"><span style="color:#E6EDF3">        start_time </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> time.time()</span></span>
<span class="line"><span style="color:#E6EDF3">        </span></span>
<span class="line"><span style="color:#FF7B72">        try</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#79C0FF">            self</span><span style="color:#E6EDF3">.logger.info(</span></span>
<span class="line"><span style="color:#A5D6FF">                "chain_started"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">                request_id</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">request_id,</span></span>
<span class="line"><span style="color:#FFA657">                input_data</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">input_data</span></span>
<span class="line"><span style="color:#E6EDF3">            )</span></span>
<span class="line"><span style="color:#E6EDF3">            </span></span>
<span class="line"><span style="color:#E6EDF3">            result </span><span style="color:#FF7B72">=</span><span style="color:#FF7B72"> await</span><span style="color:#79C0FF"> self</span><span style="color:#E6EDF3">.chain.ainvoke(input_data)</span></span>
<span class="line"><span style="color:#E6EDF3">            </span></span>
<span class="line"><span style="color:#E6EDF3">            duration </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> time.time() </span><span style="color:#FF7B72">-</span><span style="color:#E6EDF3"> start_time</span></span>
<span class="line"><span style="color:#E6EDF3">            request_counter.inc()</span></span>
<span class="line"><span style="color:#E6EDF3">            request_duration.observe(duration)</span></span>
<span class="line"><span style="color:#E6EDF3">            </span></span>
<span class="line"><span style="color:#79C0FF">            self</span><span style="color:#E6EDF3">.logger.info(</span></span>
<span class="line"><span style="color:#A5D6FF">                "chain_completed"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">                request_id</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">request_id,</span></span>
<span class="line"><span style="color:#FFA657">                duration</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">duration</span></span>
<span class="line"><span style="color:#E6EDF3">            )</span></span>
<span class="line"><span style="color:#E6EDF3">            </span></span>
<span class="line"><span style="color:#FF7B72">            return</span><span style="color:#E6EDF3"> result</span></span>
<span class="line"><span style="color:#E6EDF3">            </span></span>
<span class="line"><span style="color:#FF7B72">        except</span><span style="color:#79C0FF"> Exception</span><span style="color:#FF7B72"> as</span><span style="color:#E6EDF3"> e:</span></span>
<span class="line"><span style="color:#79C0FF">            self</span><span style="color:#E6EDF3">.logger.error(</span></span>
<span class="line"><span style="color:#A5D6FF">                "chain_failed"</span><span style="color:#E6EDF3">,</span></span>
<span class="line"><span style="color:#FFA657">                request_id</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">request_id,</span></span>
<span class="line"><span style="color:#FFA657">                error</span><span style="color:#FF7B72">=</span><span style="color:#79C0FF">str</span><span style="color:#E6EDF3">(e)</span></span>
<span class="line"><span style="color:#E6EDF3">            )</span></span>
<span class="line"><span style="color:#FF7B72">            raise</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Environment-specific configuration</span></span>
<span class="line"><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> os</span></span>
<span class="line"><span style="color:#FF7B72">from</span><span style="color:#E6EDF3"> pydantic </span><span style="color:#FF7B72">import</span><span style="color:#E6EDF3"> BaseSettings</span></span>
<span class="line"></span>
<span class="line"><span style="color:#FF7B72">class</span><span style="color:#FFA657"> Settings</span><span style="color:#E6EDF3">(</span><span style="color:#79C0FF">BaseSettings</span><span style="color:#E6EDF3">):</span></span>
<span class="line"><span style="color:#E6EDF3">    openai_api_key: </span><span style="color:#79C0FF">str</span></span>
<span class="line"><span style="color:#E6EDF3">    model_name: </span><span style="color:#79C0FF">str</span><span style="color:#FF7B72"> =</span><span style="color:#A5D6FF"> "gpt-3.5-turbo"</span></span>
<span class="line"><span style="color:#E6EDF3">    temperature: </span><span style="color:#79C0FF">float</span><span style="color:#FF7B72"> =</span><span style="color:#79C0FF"> 0.7</span></span>
<span class="line"><span style="color:#E6EDF3">    max_tokens: </span><span style="color:#79C0FF">int</span><span style="color:#FF7B72"> =</span><span style="color:#79C0FF"> 500</span></span>
<span class="line"><span style="color:#E6EDF3">    redis_url: </span><span style="color:#79C0FF">str</span><span style="color:#FF7B72"> =</span><span style="color:#A5D6FF"> "redis://localhost:6379"</span></span>
<span class="line"><span style="color:#E6EDF3">    </span></span>
<span class="line"><span style="color:#FF7B72">    class</span><span style="color:#FFA657"> Config</span><span style="color:#E6EDF3">:</span></span>
<span class="line"><span style="color:#E6EDF3">        env_file </span><span style="color:#FF7B72">=</span><span style="color:#A5D6FF"> ".env"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E6EDF3">settings </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> Settings()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#8B949E"># Initialize with settings</span></span>
<span class="line"><span style="color:#E6EDF3">llm </span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3"> ChatOpenAI(</span></span>
<span class="line"><span style="color:#FFA657">    api_key</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">settings.openai_api_key,</span></span>
<span class="line"><span style="color:#FFA657">    model</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">settings.model_name,</span></span>
<span class="line"><span style="color:#FFA657">    temperature</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">settings.temperature,</span></span>
<span class="line"><span style="color:#FFA657">    max_tokens</span><span style="color:#FF7B72">=</span><span style="color:#E6EDF3">settings.max_tokens</span></span>
<span class="line"><span style="color:#E6EDF3">)</span></span>
<span class="line"></span></code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>LangChain enables building powerful LLM applications. Key takeaways:</p>
<ol>
<li><strong>Start Simple</strong> - Begin with basic chains, add complexity gradually</li>
<li><strong>Use RAG</strong> - Enhance responses with relevant context</li>
<li><strong>Implement Agents</strong> - Let LLMs use tools intelligently</li>
<li><strong>Evaluate Rigorously</strong> - Test quality with proper metrics</li>
<li><strong>Monitor Production</strong> - Track usage, costs, and performance</li>
</ol>
<p><strong>Remember</strong>: Building with LLMs is iterative - experiment, evaluate, refine.</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="https://python.langchain.com/">LangChain Documentation</a></li>
<li><a href="https://github.com/langchain-ai/langchain/tree/master/cookbook">LangChain Cookbook</a></li>
<li><a href="https://smith.langchain.com/">LangSmith</a> - Monitoring platform</li>
<li><a href="https://smith.langchain.com/hub">LangChain Hub</a> - Prompt templates</li>
</ul>
<hr>
<p><em>What LangChain patterns work best for your use case? Share your experience!</em></p> </div> </article> </div> <div class="back-link" data-astro-cid-rkg3zjxi><a href="/ai-news/" data-astro-cid-rkg3zjxi>← Back to AI News</a></div>  </main> <footer class="site-footer container"> <p>© 2025 AREZKI El Mehdi • <a href="https://github.com/earezki">GitHub</a> • <a href="/rss.xml">RSS</a></p> </footer>  <script>
      // Read/Unread article tracking
      (function() {
        const STORAGE_KEY = 'readArticles';
        
        function getReadArticles() {
          try {
            const data = localStorage.getItem(STORAGE_KEY);
            return data ? JSON.parse(data) : [];
          } catch (e) {
            return [];
          }
        }
        
        function markArticleAsRead(url) {
          try {
            const readArticles = getReadArticles();
            if (!readArticles.includes(url)) {
              readArticles.push(url);
              localStorage.setItem(STORAGE_KEY, JSON.stringify(readArticles));
            }
          } catch (e) {
            console.error('Failed to mark article as read:', e);
          }
        }
        
        function isArticleRead(url) {
          const readArticles = getReadArticles();
          return readArticles.includes(url);
        }
        
        // Update post cards on list pages
        function updatePostCards() {
          const postCards = document.querySelectorAll('.post-card[data-article-url]');
          postCards.forEach(function(card) {
            const url = card.getAttribute('data-article-url');
            if (url) {
              if (isArticleRead(url)) {
                card.classList.add('read');
                card.classList.remove('unread');
              } else {
                card.classList.add('unread');
                card.classList.remove('read');
              }
            }
          });
        }
        
        // Mark current article as read (on article pages)
        function markCurrentArticleAsRead() {
          const isArticlePage = document.querySelector('.post-body');
          if (isArticlePage) {
            const currentPath = window.location.pathname;
            markArticleAsRead(currentPath);
          }
        }
        
        // Initialize function
        function init() {
          updatePostCards();
          markCurrentArticleAsRead();
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', init);
        
        // Handle back/forward navigation (bfcache)
        window.addEventListener('pageshow', function(event) {
          // If page is loaded from cache, update the cards
          if (event.persisted) {
            init();
          }
        });
      })();
    </script> </body> </html> 