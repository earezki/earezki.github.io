<!DOCTYPE html><html lang="en" data-theme="dark"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Why Observability Matters for AI Applications: A Deep Dive into LLM Monitoring • Dev|Journal</title><meta name="description" content="Main Heading: Observability in the Age of AI: Addressing the Unique Challenges of LLMs This presentation by Sally O'Malley from Red Hat delves into the…"><meta property="og:title" content="Why Observability Matters for AI Applications: A Deep Dive into LLM Monitoring • Dev|Journal"><meta property="og:site_name" content="Dev|Journal"><meta property="og:description" content="Main Heading: Observability in the Age of AI: Addressing the Unique Challenges of LLMs This presentation by Sally O'Malley from Red Hat delves into the…"><link rel="canonical" href="https://earezki.com/ai-news/2025-10-20-presentation-why-observability-matters-more-with-ai-applications/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Why Observability Matters for AI Applications: A Deep Dive into LLM Monitoring • Dev|Journal"><meta name="twitter:description" content="Main Heading: Observability in the Age of AI: Addressing the Unique Challenges of LLMs This presentation by Sally O'Malley from Red Hat delves into the…"><link rel="alternate" type="application/rss+xml" title="RSS" href="/rss.xml"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-161447264-1"></script><script defer src="https://cloud.umami.is/script.js" data-website-id="4a26531d-1053-4f79-97a6-06a1366aff91"></script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Why Observability Matters for AI Applications: A Deep Dive into LLM Monitoring","datePublished":"2025-10-20T00:00:00.000Z","dateModified":"2025-10-20T00:00:00.000Z","author":{"@type":"Person","name":"AREZKI El Mehdi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://earezki.com/ai-news/2025-10-20-presentation-why-observability-matters-more-with-ai-applications/"},"description":"Main Heading: Observability in the Age of AI: Addressing the Unique Challenges of LLMs This presentation by Sally O'Malley from Red Hat delves into the…"}</script><link rel="stylesheet" href="/_astro/_slug_.C0R72pw6.css">
<style>.post-layout[data-astro-cid-rkg3zjxi]{display:grid;grid-template-columns:260px 1fr;gap:2.5rem}.sidebar-left[data-astro-cid-rkg3zjxi]{position:relative}@media (max-width: 1080px){.post-layout[data-astro-cid-rkg3zjxi]{grid-template-columns:1fr}}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]{display:flex;gap:.75rem;padding:.9rem 1.1rem;margin:0 auto 2rem;max-width:780px;background:linear-gradient(135deg,#667eea1a,#764ba21a,#f093fb1a);border:2px solid transparent;border-radius:var(--radius-md);position:relative;background-clip:padding-box;font-size:.85rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]:before{content:"";position:absolute;inset:-2px;border-radius:var(--radius-md);padding:2px;background:var(--ai-gradient-border);background-size:200% 200%;animation:gradient-rotate 3s linear infinite;-webkit-mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);-webkit-mask-composite:xor;mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);mask-composite:exclude;pointer-events:none}html[data-theme=dark] .ai-disclaimer-article[data-astro-cid-rkg3zjxi]{background:linear-gradient(135deg,#667eea26,#764ba226,#f093fb26)}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] svg[data-astro-cid-rkg3zjxi]{margin-top:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] strong[data-astro-cid-rkg3zjxi]{color:var(--color-text);display:block;margin-bottom:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] div[data-astro-cid-rkg3zjxi]{line-height:1.5;color:var(--color-text-alt)}
.toc[data-astro-cid-xvrfupwn]{position:sticky;top:90px;max-height:calc(100vh - 120px);overflow:auto;padding:1rem 1rem 1.2rem;background:var(--color-bg-alt);border:1px solid var(--color-border);border-radius:var(--radius-md);font-size:.8rem;line-height:1.3}.toc-title[data-astro-cid-xvrfupwn]{font-weight:600;font-size:.75rem;text-transform:uppercase;letter-spacing:.08em;margin-bottom:.6rem;color:var(--color-text-alt)}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;margin:0;padding:0;display:flex;flex-direction:column;gap:.35rem}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{text-decoration:none;color:var(--color-text-alt)}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--color-accent)}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-3]{margin-left:.75rem}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-4]{margin-left:1.4rem}@media (max-width: 1080px){.toc[data-astro-cid-xvrfupwn]{display:none}}
</style><script type="module" src="/_astro/hoisted.CwSnpN4J.js"></script></head> <body> <div id="readingProgress" style="position:fixed;left:0;top:0;height:3px;background:linear-gradient(90deg,#2563eb,#9333ea);width:0;z-index:999;transition:width .15s ease;"></div>  <header class="site-header container"> <div class="logo-wrap"> <a class="logo" href="/">Dev|Journal</a> </div> <nav class="main-nav"> <a href="/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path> <polyline points="9 22 9 12 15 12 15 22"></polyline> </svg>
Home
</a> <a href="/ai-news/" class="ai-news-link"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h-9.5a.5.5 0 0 0-.5.5.5.5 0 0 1-1 0 .5.5 0 0 0-.5-.5H1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2z"></path> <path d="M7 15v4a2 2 0 0 0 2 2h6a2 2 0 0 0 2-2v-4"></path> </svg>
AI News
</a> <a href="/tags/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2H2v10l9.29 9.29c.94.94 2.48.94 3.42 0l6.58-6.58c.94-.94.94-2.48 0-3.42L12 2z"></path> <circle cx="7" cy="7" r="1.5"></circle> </svg>
Tags
</a> <a href="/about/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <circle cx="12" cy="12" r="10"></circle> <path d="M12 16v-4"></path> <path d="M12 8h.01"></path> </svg>
About
</a> <button id="themeToggle" class="theme-toggle" aria-label="Toggle dark mode"> <span class="theme-icon">☾</span> <span class="theme-text">dark</span> </button> </nav> </header> <main class="container content-area">  <div class="ai-disclaimer-article" data-astro-cid-rkg3zjxi> <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="flex-shrink: 0;" data-astro-cid-rkg3zjxi> <circle cx="12" cy="12" r="10" data-astro-cid-rkg3zjxi></circle> <line x1="12" y1="8" x2="12" y2="12" data-astro-cid-rkg3zjxi></line> <line x1="12" y1="16" x2="12.01" y2="16" data-astro-cid-rkg3zjxi></line> </svg> <div data-astro-cid-rkg3zjxi> <strong data-astro-cid-rkg3zjxi>AI-Generated Content:</strong> This article was created with AI assistance. Please verify important information and check original references.
</div> </div> <div class="post-layout" data-astro-cid-rkg3zjxi> <aside class="sidebar-left" data-astro-cid-rkg3zjxi> <nav class="toc" aria-label="Table of Contents" data-astro-cid-xvrfupwn> <div class="toc-title" data-astro-cid-xvrfupwn>On this page</div> <ul data-astro-cid-xvrfupwn> <li class="d-2" data-astro-cid-xvrfupwn><a href="#main-heading-observability-in-the-age-of-ai-addressing-the-unique-challenges-of-llms" data-astro-cid-xvrfupwn>Main Heading: Observability in the Age of AI: Addressing the Unique Challenges of LLMs</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#summary" data-astro-cid-xvrfupwn>Summary</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#detailed-breakdown" data-astro-cid-xvrfupwn>Detailed Breakdown</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#1-introduction-the-rise-of-ai-and-the-need-for-observability" data-astro-cid-xvrfupwn>1. Introduction: The Rise of AI and the Need for Observability</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#2-why-llms-pose-unique-challenges" data-astro-cid-xvrfupwn>2. Why LLMs Pose Unique Challenges</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#3-building-an-open-source-observability-stack" data-astro-cid-xvrfupwn>3. Building an Open-Source Observability Stack</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#4-key-metrics-to-monitor" data-astro-cid-xvrfupwn>4. Key Metrics to Monitor</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#5-practical-demonstration" data-astro-cid-xvrfupwn>5.  Practical Demonstration</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#6-qa-and-discussion" data-astro-cid-xvrfupwn>6.  Q&amp;A and Discussion</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#conclusion" data-astro-cid-xvrfupwn>Conclusion</a></li><li class="d-2" data-astro-cid-xvrfupwn><a href="#references" data-astro-cid-xvrfupwn>References</a></li> </ul> </nav>  </aside> <article class="post" data-astro-cid-rkg3zjxi> <header class="post-header" data-astro-cid-rkg3zjxi> <h1 data-astro-cid-rkg3zjxi>Why Observability Matters for AI Applications: A Deep Dive into LLM Monitoring</h1> <div class="meta" data-astro-cid-rkg3zjxi> <time datetime="2025-10-20T00:00:00.000Z" data-astro-cid-rkg3zjxi>Mon Oct 20 2025</time> <span data-astro-cid-rkg3zjxi>• 4 min read</span> </div> <div class="tag-row" data-astro-cid-rkg3zjxi><a class="tag ai-news-tag" href="/ai-news/" data-astro-cid-rkg3zjxi>AI News</a><span class="tag" data-astro-cid-rkg3zjxi>Observability</span><span class="tag" data-astro-cid-rkg3zjxi>LLM</span><span class="tag" data-astro-cid-rkg3zjxi>Kubernetes</span><span class="tag" data-astro-cid-rkg3zjxi>Monitoring</span></div> </header> <div class="post-body prose" data-astro-cid-rkg3zjxi> <h2 id="main-heading-observability-in-the-age-of-ai-addressing-the-unique-challenges-of-llms">Main Heading: Observability in the Age of AI: Addressing the Unique Challenges of LLMs</h2>
<p>This presentation by Sally O’Malley from Red Hat delves into the critical importance of observability for modern AI applications, particularly Large Language Models (LLMs).  It highlights the unique challenges LLMs present compared to traditional applications and provides a practical, hands-on guide to building an open-source observability stack.  The talk emphasizes the need for monitoring performance, cost, and quality to ensure the reliability and effectiveness of AI-powered workloads.</p>
<h2 id="summary">Summary</h2>
<p>This presentation addresses the growing need for observability in AI applications, focusing specifically on the challenges posed by LLMs.  O’Malley outlines the characteristics of LLMs that make them distinct from traditional applications, such as non-uniformity, variable latency, and cost considerations.  She then details a comprehensive approach to building an observability stack using open-source tools like vLLM, Llama Stack, Prometheus, Grafana, and OpenTelemetry.  The presentation covers key metrics to monitor, including performance (latency), cost (token usage), and quality (tool utilization, accuracy).  Finally, O’Malley demonstrates how to implement this stack and provides insights into the benefits of tracing for debugging and understanding AI model behavior.  The discussion also covers considerations for different roles within an organization (developers, SREs, etc.) and the importance of tools like Llama Stack for managing and monitoring AI applications.</p>
<h2 id="detailed-breakdown">Detailed Breakdown</h2>
<h3 id="1-introduction-the-rise-of-ai-and-the-need-for-observability">1. Introduction: The Rise of AI and the Need for Observability</h3>
<ul>
<li><strong>Context:</strong> The presentation begins by emphasizing the rapid growth of AI adoption across industries.  85% of executives recognize AI’s importance, yet 75% lack the expertise to implement it effectively.</li>
<li><strong>The Observability Challenge:</strong>  Traditional observability practices are insufficient for LLMs due to their inherent characteristics.  The talk highlights the need for specialized monitoring techniques.</li>
<li><strong>Focus:</strong> The presentation centers on providing a practical, open-source solution for monitoring AI workloads.</li>
</ul>
<h3 id="2-why-llms-pose-unique-challenges">2. Why LLMs Pose Unique Challenges</h3>
<ul>
<li><strong>Non-Uniformity:</strong> LLMs exhibit variable response times, making traditional metrics less reliable.</li>
<li><strong>Cost:</strong>  Running LLMs can be expensive, especially with GPU-intensive models.  Token usage is a primary cost driver.</li>
<li><strong>Complexity:</strong>  LLM workflows involve multiple stages (prefill, decode, agents), requiring granular monitoring.</li>
<li><strong>Dynamic Behavior:</strong>  LLM outputs can vary significantly even with the same input, necessitating robust tracking.</li>
</ul>
<h3 id="3-building-an-open-source-observability-stack">3. Building an Open-Source Observability Stack</h3>
<ul>
<li><strong>Core Components:</strong> The recommended stack includes:
<ul>
<li><strong>vLLM:</strong> A fast and easy-to-use library for LLM inference.</li>
<li><strong>Llama Stack:</strong> A framework for building AI applications, providing components like RAG, agents, and a unified API.</li>
<li><strong>Prometheus:</strong> A metrics backend for collecting and storing time-series data.</li>
<li><strong>Grafana:</strong> A visualization tool for creating dashboards and analyzing metrics.</li>
<li><strong>OpenTelemetry:</strong> A standard for collecting telemetry data (metrics, traces, logs).</li>
<li><strong>Tempo:</strong> A tracing backend for analyzing request flows and identifying bottlenecks.</li>
</ul>
</li>
<li><strong>Deployment:</strong> The presentation focuses on deploying this stack using Kubernetes, leveraging tools like Minikube for local development.</li>
<li><strong>Llama Stack and vLLM Integration:</strong> Llama Stack simplifies the deployment and management of LLMs, providing tools for RAG, agents, and a unified API.  vLLM is used for efficient model inference.</li>
<li><strong>OpenTelemetry Collector:</strong>  This component collects telemetry data from applications and forwards it to the chosen backend (Prometheus, Tempo).</li>
<li><strong>ServiceMonitors:</strong>  These Kubernetes resources automatically configure Prometheus to scrape metrics from services.</li>
<li><strong>Example Queries:</strong>  The presenter demonstrates how to use Grafana to query metrics related to GPU utilization, model usage, and latency.</li>
</ul>
<h3 id="4-key-metrics-to-monitor">4. Key Metrics to Monitor</h3>
<ul>
<li><strong>Performance Metrics:</strong>
<ul>
<li><strong>Latency:</strong> Time taken for different stages of the LLM workflow (prefill, decode).</li>
<li><strong>Throughput:</strong>  The number of requests processed per unit of time.</li>
</ul>
</li>
<li><strong>Cost Metrics:</strong>
<ul>
<li><strong>Token Usage:</strong>  The number of tokens processed by the model, directly impacting cost.</li>
<li><strong>GPU Utilization:</strong>  The amount of GPU resources consumed.</li>
</ul>
</li>
<li><strong>Quality Metrics:</strong>
<ul>
<li><strong>Tool Utilization:</strong>  Whether the LLM is using the intended tools.</li>
<li><strong>Accuracy:</strong>  The correctness of the LLM’s responses (requires integration with evaluation metrics).</li>
</ul>
</li>
<li><strong>Tracing:</strong>
<ul>
<li><strong>End-to-End Traces:</strong>  Visualize the entire request flow, identifying bottlenecks and latency hotspots.</li>
<li><strong>Context:</strong>  Understand the context of each request and its dependencies.</li>
</ul>
</li>
</ul>
<h3 id="5--practical-demonstration">5.  Practical Demonstration</h3>
<ul>
<li><strong>Live Demo:</strong> The presentation includes a live demonstration of the observability stack in action.</li>
<li><strong>Real-time Monitoring:</strong> The presenter shows how to monitor GPU utilization, track token usage, and analyze traces in real-time.</li>
<li><strong>Interactive Exploration:</strong> The audience is encouraged to explore the dashboards and experiment with different queries.</li>
</ul>
<h3 id="6--qa-and-discussion">6.  Q&#x26;A and Discussion</h3>
<ul>
<li><strong>Participant Questions:</strong> The Q&#x26;A session addresses questions regarding cost, tool choices, and the suitability of observability for different roles.</li>
<li><strong>Key Takeaways:</strong>  The discussion reinforces the importance of observability for ensuring the reliability, efficiency, and cost-effectiveness of LLM-powered applications.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>The presentation advocates for a proactive approach to observability in the age of AI, particularly for LLMs. By leveraging open-source tools and focusing on key metrics, organizations can gain valuable insights into their AI workloads, optimize performance, control costs, and ensure the delivery of high-quality results.  The choice between tools like Dynatrace and open-source options like Grafana and Tempo depends on specific needs and priorities.  The presentation emphasizes the importance of tracing for understanding complex AI workflows and debugging issues.</p>
<h2 id="references">References</h2>
<ul>
<li><strong>Original Presentation:</strong> <a href="https://www.infoq.com/presentations/observability-llm/">https://www.infoq.com/presentations/observability-llm/</a></li>
<li><strong>Llama Stack:</strong> <a href="https://github.com/meta-llama/Llama-Stack">https://github.com/meta-llama/Llama-Stack</a></li>
<li><strong>vLLM:</strong> <a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></li>
<li><strong>Minikube:</strong> <a href="https://minikube.sigs.k8s.io/docs/">https://minikube.sigs.k8s.io/docs/</a></li>
<li><strong>OpenTelemetry:</strong> <a href="https://opentelemetry.io/">https://opentelemetry.io/</a></li>
<li><strong>Grafana:</strong> <a href="https://grafana.com/">https://grafana.com/</a></li>
<li><strong>Prometheus:</strong> <a href="https://prometheus.io/">https://prometheus.io/</a></li>
<li><strong>InfoQ Dev Summit Boston 2025:</strong> <a href="https://www.infoq.com/events/devsummit-boston-2025/">https://www.infoq.com/events/devsummit-boston-2025/</a></li>
</ul> </div> </article> </div> <div class="back-link" data-astro-cid-rkg3zjxi><a href="/ai-news/" data-astro-cid-rkg3zjxi>← Back to AI News</a></div>  </main> <footer class="site-footer container"> <p>© 2025 AREZKI El Mehdi • <a href="https://github.com/earezki">GitHub</a> • <a href="/rss.xml">RSS</a></p> </footer>  <script>
      // Read/Unread article tracking
      (function() {
        const STORAGE_KEY = 'readArticles';
        
        function getReadArticles() {
          try {
            const data = localStorage.getItem(STORAGE_KEY);
            return data ? JSON.parse(data) : [];
          } catch (e) {
            return [];
          }
        }
        
        function markArticleAsRead(url) {
          try {
            const readArticles = getReadArticles();
            if (!readArticles.includes(url)) {
              readArticles.push(url);
              localStorage.setItem(STORAGE_KEY, JSON.stringify(readArticles));
            }
          } catch (e) {
            console.error('Failed to mark article as read:', e);
          }
        }
        
        function isArticleRead(url) {
          const readArticles = getReadArticles();
          return readArticles.includes(url);
        }
        
        // Update post cards on list pages
        function updatePostCards() {
          const postCards = document.querySelectorAll('.post-card[data-article-url]');
          postCards.forEach(function(card) {
            const url = card.getAttribute('data-article-url');
            if (url) {
              if (isArticleRead(url)) {
                card.classList.add('read');
                card.classList.remove('unread');
              } else {
                card.classList.add('unread');
                card.classList.remove('read');
              }
            }
          });
        }
        
        // Mark current article as read (on article pages)
        function markCurrentArticleAsRead() {
          const isArticlePage = document.querySelector('.post-body');
          if (isArticlePage) {
            const currentPath = window.location.pathname;
            markArticleAsRead(currentPath);
          }
        }
        
        // Initialize function
        function init() {
          updatePostCards();
          markCurrentArticleAsRead();
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', init);
        
        // Handle back/forward navigation (bfcache)
        window.addEventListener('pageshow', function(event) {
          // If page is loaded from cache, update the cards
          if (event.persisted) {
            init();
          }
        });
      })();
    </script> </body> </html> 