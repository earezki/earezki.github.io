<!DOCTYPE html><html lang="en" data-theme="dark"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>IBM Advances Open-Source AI with vLLM, torch.compile, and Spyre Accelerator Integration • Dev|Journal</title><meta name="description" content="Summary of IBM's Contributions to Open-Source AI IBM is actively expanding its contributions to the open-source AI community, particularly within the PyTorch…"><meta property="og:title" content="IBM Advances Open-Source AI with vLLM, torch.compile, and Spyre Accelerator Integration • Dev|Journal"><meta property="og:site_name" content="Dev|Journal"><meta property="og:description" content="Summary of IBM's Contributions to Open-Source AI IBM is actively expanding its contributions to the open-source AI community, particularly within the PyTorch…"><link rel="canonical" href="https://earezki.com/ai-news/2025-10-21-ibm-is-expanding-open-source-ai-model-training-and-inference/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="IBM Advances Open-Source AI with vLLM, torch.compile, and Spyre Accelerator Integration • Dev|Journal"><meta name="twitter:description" content="Summary of IBM's Contributions to Open-Source AI IBM is actively expanding its contributions to the open-source AI community, particularly within the PyTorch…"><link rel="alternate" type="application/rss+xml" title="RSS" href="/rss.xml"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-161447264-1"></script><script defer src="https://cloud.umami.is/script.js" data-website-id="4a26531d-1053-4f79-97a6-06a1366aff91"></script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"IBM Advances Open-Source AI with vLLM, torch.compile, and Spyre Accelerator Integration","datePublished":"2021-02-09T00:00:00.000Z","dateModified":"2021-02-09T00:00:00.000Z","author":{"@type":"Person","name":"AREZKI El Mehdi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://earezki.com/ai-news/2025-10-21-ibm-is-expanding-open-source-ai-model-training-and-inference/"},"description":"Summary of IBM's Contributions to Open-Source AI IBM is actively expanding its contributions to the open-source AI community, particularly within the PyTorch…"}</script><link rel="stylesheet" href="/_astro/_slug_.Cydfb2eQ.css">
<style>.post-layout[data-astro-cid-rkg3zjxi]{display:grid;grid-template-columns:260px 1fr;gap:2.5rem}.sidebar-left[data-astro-cid-rkg3zjxi]{position:relative}@media (max-width: 1080px){.post-layout[data-astro-cid-rkg3zjxi]{grid-template-columns:1fr}}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]{display:flex;gap:.75rem;padding:.9rem 1.1rem;margin:0 auto 2rem;max-width:780px;background:linear-gradient(135deg,#667eea1a,#764ba21a,#f093fb1a);border:2px solid transparent;border-radius:var(--radius-md);position:relative;background-clip:padding-box;font-size:.85rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi]:before{content:"";position:absolute;inset:-2px;border-radius:var(--radius-md);padding:2px;background:var(--ai-gradient-border);background-size:200% 200%;animation:gradient-rotate 3s linear infinite;-webkit-mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);-webkit-mask-composite:xor;mask:linear-gradient(#fff 0 0) content-box,linear-gradient(#fff 0 0);mask-composite:exclude;pointer-events:none}html[data-theme=dark] .ai-disclaimer-article[data-astro-cid-rkg3zjxi]{background:linear-gradient(135deg,#667eea26,#764ba226,#f093fb26)}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] svg[data-astro-cid-rkg3zjxi]{margin-top:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] strong[data-astro-cid-rkg3zjxi]{color:var(--color-text);display:block;margin-bottom:.15rem}.ai-disclaimer-article[data-astro-cid-rkg3zjxi] div[data-astro-cid-rkg3zjxi]{line-height:1.5;color:var(--color-text-alt)}
.toc[data-astro-cid-xvrfupwn]{position:sticky;top:90px;max-height:calc(100vh - 120px);overflow:auto;padding:1rem 1rem 1.2rem;background:var(--color-bg-alt);border:1px solid var(--color-border);border-radius:var(--radius-md);font-size:.8rem;line-height:1.3}.toc-title[data-astro-cid-xvrfupwn]{font-weight:600;font-size:.75rem;text-transform:uppercase;letter-spacing:.08em;margin-bottom:.6rem;color:var(--color-text-alt)}.toc[data-astro-cid-xvrfupwn] ul[data-astro-cid-xvrfupwn]{list-style:none;margin:0;padding:0;display:flex;flex-direction:column;gap:.35rem}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]{text-decoration:none;color:var(--color-text-alt)}.toc[data-astro-cid-xvrfupwn] a[data-astro-cid-xvrfupwn]:hover{color:var(--color-accent)}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-3]{margin-left:.75rem}.toc[data-astro-cid-xvrfupwn] li[data-astro-cid-xvrfupwn][class*=d-4]{margin-left:1.4rem}@media (max-width: 1080px){.toc[data-astro-cid-xvrfupwn]{display:none}}
</style><script type="module" src="/_astro/hoisted.CwSnpN4J.js"></script></head> <body> <div id="readingProgress" style="position:fixed;left:0;top:0;height:3px;background:linear-gradient(90deg,#2563eb,#9333ea);width:0;z-index:999;transition:width .15s ease;"></div>  <header class="site-header container"> <div class="logo-wrap"> <a class="logo" href="/">Dev|Journal</a> </div> <nav class="main-nav"> <a href="/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path> <polyline points="9 22 9 12 15 12 15 22"></polyline> </svg>
Home
</a> <a href="/ai-news/" class="ai-news-link"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h-9.5a.5.5 0 0 0-.5.5.5.5 0 0 1-1 0 .5.5 0 0 0-.5-.5H1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2z"></path> <path d="M7 15v4a2 2 0 0 0 2 2h6a2 2 0 0 0 2-2v-4"></path> </svg>
AI News
</a> <a href="/tags/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <path d="M12 2H2v10l9.29 9.29c.94.94 2.48.94 3.42 0l6.58-6.58c.94-.94.94-2.48 0-3.42L12 2z"></path> <circle cx="7" cy="7" r="1.5"></circle> </svg>
Tags
</a> <a href="/about/"> <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="vertical-align: -2px; margin-right: 4px;"> <circle cx="12" cy="12" r="10"></circle> <path d="M12 16v-4"></path> <path d="M12 8h.01"></path> </svg>
About
</a> <button id="themeToggle" class="theme-toggle" aria-label="Toggle dark mode"> <span class="theme-icon">☾</span> <span class="theme-text">dark</span> </button> </nav> </header> <main class="container content-area">  <div class="ai-disclaimer-article" data-astro-cid-rkg3zjxi> <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="flex-shrink: 0;" data-astro-cid-rkg3zjxi> <circle cx="12" cy="12" r="10" data-astro-cid-rkg3zjxi></circle> <line x1="12" y1="8" x2="12" y2="12" data-astro-cid-rkg3zjxi></line> <line x1="12" y1="16" x2="12.01" y2="16" data-astro-cid-rkg3zjxi></line> </svg> <div data-astro-cid-rkg3zjxi> <strong data-astro-cid-rkg3zjxi>AI-Generated Content:</strong> This article was created with AI assistance. Please verify important information and check original references.
</div> </div> <div class="post-layout" data-astro-cid-rkg3zjxi> <aside class="sidebar-left" data-astro-cid-rkg3zjxi> <nav class="toc" aria-label="Table of Contents" data-astro-cid-xvrfupwn> <div class="toc-title" data-astro-cid-xvrfupwn>On this page</div> <ul data-astro-cid-xvrfupwn> <li class="d-2" data-astro-cid-xvrfupwn><a href="#summary-of-ibms-contributions-to-open-source-ai" data-astro-cid-xvrfupwn>Summary of IBM&#39;s Contributions to Open-Source AI</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#improving-vllm-with-hardware-agnostic-kernels" data-astro-cid-xvrfupwn>Improving vLLM with Hardware-Agnostic Kernels</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#a-new-model-training-milestone-with-torchtitan" data-astro-cid-xvrfupwn>A New Model Training Milestone with torchtitan</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#integration-of-spyre-ai-accelerator-with-vllm-and-torchcompile" data-astro-cid-xvrfupwn>Integration of Spyre AI Accelerator with vLLM and torch.compile</a></li><li class="d-3" data-astro-cid-xvrfupwn><a href="#conclusion" data-astro-cid-xvrfupwn>Conclusion</a></li> </ul> </nav>  </aside> <article class="post" data-astro-cid-rkg3zjxi> <header class="post-header" data-astro-cid-rkg3zjxi> <h1 data-astro-cid-rkg3zjxi>IBM Advances Open-Source AI with vLLM, torch.compile, and Spyre Accelerator Integration</h1> <div class="meta" data-astro-cid-rkg3zjxi> <time datetime="2021-02-09T00:00:00.000Z" data-astro-cid-rkg3zjxi>Tue Feb 09 2021</time> <span data-astro-cid-rkg3zjxi>• 3 min read</span> </div> <div class="tag-row" data-astro-cid-rkg3zjxi><a class="tag ai-news-tag" href="/ai-news/" data-astro-cid-rkg3zjxi>AI News</a><span class="tag" data-astro-cid-rkg3zjxi>Open Source</span><span class="tag" data-astro-cid-rkg3zjxi>Hardware Acceleration</span></div> </header> <div class="post-body prose" data-astro-cid-rkg3zjxi> <h2 id="summary-of-ibms-contributions-to-open-source-ai">Summary of IBM’s Contributions to Open-Source AI</h2>
<p>IBM is actively expanding its contributions to the open-source AI community, particularly within the PyTorch and vLLM ecosystems. At PyTorch 2025, the company showcased advancements in model training, inference, and accelerator integration, demonstrating a commitment to efficient, transparent, and community-driven AI infrastructure. These efforts involve developing hardware-agnostic kernels for vLLM, achieving significant cost and efficiency improvements in LLM training with torchtitan, and integrating its Spyre AI accelerator for enhanced inference capabilities.</p>
<h3 id="improving-vllm-with-hardware-agnostic-kernels">Improving vLLM with Hardware-Agnostic Kernels</h3>
<p><strong>Overview:</strong> IBM Research has developed Triton kernels to enhance the performance of vLLM, a popular open-source LLM inference framework. This initiative aims to provide a fully contained attention backend that works efficiently on multiple GPU platforms, including AMD, without relying on proprietary libraries like FlashAttention.</p>
<p><strong>Key Details:</strong></p>
<ul>
<li><strong>Problem:</strong> The previous version of vLLM (V1) relied on external FlashAttention libraries, limiting hardware support to NVIDIA GPUs.</li>
<li><strong>Solution:</strong> IBM researchers developed Triton kernels, a platform-independent framework for writing GPU kernels, to enable vLLM to run efficiently on AMD GPUs.</li>
<li><strong>Performance Improvement:</strong> Through extensive kernel-level optimizations, IBM achieved a more than 10% improvement in token throughput for vLLM V1 on AMD hardware compared to vLLM V0.</li>
<li><strong>Technical Details:</strong> The team addressed performance bottlenecks arising from the scheduler’s interleaving of prefill and decode requests, leading to significant optimizations for models using grouped query attention, such as IBM’s Granite 3.0 family.</li>
<li><strong>Community Benefit:</strong> The newly developed attention backend is self-contained, doesn’t depend on external libraries, and is maintained by the vLLM team, allowing for flexibility in adapting to new models.</li>
<li><strong>Future Work:</strong> IBM is working on reducing the launch overhead of these Triton kernels and expanding optimizations to other model architectures, including IBM’s Granite 4, which utilizes Mamba layers. Meta’s PyTorch Team is also announcing a public beta for Helion, a domain-specific language for kernel development, which compiles down to Triton.</li>
</ul>
<h3 id="a-new-model-training-milestone-with-torchtitan">A New Model Training Milestone with torchtitan</h3>
<p><strong>Overview:</strong> IBM Research has successfully trained a new branch of the Llama 3 70B model using torchtitan, a new PyTorch-native training framework. This achievement demonstrates the framework’s capability to achieve production-grade efficiency with significantly reduced resources.</p>
<p><strong>Key Details:</strong></p>
<ul>
<li><strong>Switch to torchtitan:</strong> IBM’s research scientist Linsong Chu’s team transitioned from a proprietary software stack to torchtitan to actively participate in the open-source community and streamline training workflows.</li>
<li><strong>Resource Efficiency:</strong> The team achieved the same model quality with only one-third of the training budget compared to the original Llama 3 training, attributed to careful data curation and training in FP8 precision.</li>
<li><strong>FP8 Precision:</strong> Utilizing FP8 precision resulted in a 1.5 times faster training speed and halved the required token count.</li>
<li><strong>torchtitan Features:</strong> The project leveraged features contributed by IBM, including a high-throughput data loader that enables efficient checkpoint saving, workload distribution, and mid-job reconfiguration.</li>
<li><strong>Community Contribution:</strong> This accomplishment showcases PyTorch’s potential for training large-scale models using open-source frameworks, with IBM actively contributing kernels and new models to the community.</li>
</ul>
<h3 id="integration-of-spyre-ai-accelerator-with-vllm-and-torchcompile">Integration of Spyre AI Accelerator with vLLM and torch.compile</h3>
<p><strong>Overview:</strong> IBM is integrating its Spyre AI accelerator with the PyTorch ecosystem, specifically with vLLM and torch.compile, to enhance inference performance and expand the reach of its hardware.</p>
<p><strong>Key Details:</strong></p>
<ul>
<li><strong>Spyre AI Accelerator:</strong> IBM’s Spyre is designed for IBM Z and Power Systems, offering a heterogeneous compute platform.</li>
<li><strong>vLLM Integration:</strong> IBM developed a new compiler that seamlessly interfaces with torch.compile, enabling vLLM to utilize the Spyre accelerator with minimal code changes.</li>
<li><strong>Paged Attention Plugin:</strong> A key component of the integration is a vLLM plugin that implements paged attention, a technique that improves memory efficiency and scalability for long LLM outputs.</li>
<li><strong>Paged Attention Mechanism:</strong> This technique divides the model’s KV cache into smaller, addressable memory blocks (“pages”) fetched on demand, reducing memory bottlenecks.</li>
<li><strong>Ease of Use:</strong> The plugin allows developers to transparently substitute vLLM endpoints with Spyre, requiring no changes to existing frameworks like BeeAI and LangChain.</li>
<li><strong>Future Plans:</strong> IBM plans to deepen the integration in 2026, focusing on tighter compiler integration, unified runtime interfaces, and expanded multi-accelerator scheduling.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>IBM’s multifaceted contributions to the open-source AI landscape, highlighted at PyTorch 2025, underscore its commitment to fostering a collaborative and innovative ecosystem. By advancing vLLM with hardware-agnostic kernels, achieving efficient training with torchtitan, and integrating its Spyre accelerator, IBM is paving the way for more powerful, accessible, and scalable AI solutions.</p>
<p><strong>Reference Link:</strong> <a href="https://research.ibm.com/blog/pytorch-expanding-ai-model-training-and-inference-for-the-open-source-community?utm_medium=rss&#x26;utm_source=rss">https://research.ibm.com/blog/pytorch-expanding-ai-model-training-and-inference-for-the-open-source-community?utm_medium=rss&#x26;utm_source=rss</a></p> </div> </article> </div> <div class="back-link" data-astro-cid-rkg3zjxi><a href="/ai-news/" data-astro-cid-rkg3zjxi>← Back to AI News</a></div>  </main> <footer class="site-footer container"> <p>© 2025 AREZKI El Mehdi • <a href="https://github.com/earezki">GitHub</a> • <a href="/rss.xml">RSS</a></p> </footer>  <script>
      // Read/Unread article tracking
      (function() {
        const STORAGE_KEY = 'readArticles';
        
        function getReadArticles() {
          try {
            const data = localStorage.getItem(STORAGE_KEY);
            return data ? JSON.parse(data) : [];
          } catch (e) {
            return [];
          }
        }
        
        function markArticleAsRead(url) {
          try {
            const readArticles = getReadArticles();
            if (!readArticles.includes(url)) {
              readArticles.push(url);
              localStorage.setItem(STORAGE_KEY, JSON.stringify(readArticles));
            }
          } catch (e) {
            console.error('Failed to mark article as read:', e);
          }
        }
        
        function isArticleRead(url) {
          const readArticles = getReadArticles();
          return readArticles.includes(url);
        }
        
        // Update post cards on list pages
        function updatePostCards() {
          const postCards = document.querySelectorAll('.post-card[data-article-url]');
          postCards.forEach(function(card) {
            const url = card.getAttribute('data-article-url');
            if (url) {
              if (isArticleRead(url)) {
                card.classList.add('read');
                card.classList.remove('unread');
              } else {
                card.classList.add('unread');
                card.classList.remove('read');
              }
            }
          });
        }
        
        // Mark current article as read (on article pages)
        function markCurrentArticleAsRead() {
          const isArticlePage = document.querySelector('.post-body');
          if (isArticlePage) {
            const currentPath = window.location.pathname;
            markArticleAsRead(currentPath);
          }
        }
        
        // Initialize function
        function init() {
          updatePostCards();
          markCurrentArticleAsRead();
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', init);
        
        // Handle back/forward navigation (bfcache)
        window.addEventListener('pageshow', function(event) {
          // If page is loaded from cache, update the cards
          if (event.persisted) {
            init();
          }
        });
      })();
    </script> </body> </html> 